{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d317eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_train.py\n",
    "\"\"\"\n",
    "Naive Bayes text classifier (Task 2)\n",
    "- Change INPUT_PATH if needed.\n",
    "- Saves outputs (model, vectorizer, reports) to the same folder as INPUT_PATH.\n",
    "Requirements:\n",
    "    pip install numpy pandas scikit-learn matplotlib joblib\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7adb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f096761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f37df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- CONFIG --------\n",
    "INPUT_PATH = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\19 naive bayes and text mining\\blogs.csv\"\n",
    "OUTPUT_FOLDER = os.path.dirname(INPUT_PATH)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e99898",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "MAX_FEATURES = 5000   # change if you want fewer/more features\n",
    "NGRAM_RANGE = (1,2)   # unigrams + bigrams\n",
    "MIN_DF = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d580e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -------- helpers --------\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text.lower()\n",
    "    t = re.sub(r\"http\\S+|www\\.\\S+\", \" \", t)        # remove urls\n",
    "    t = re.sub(r\"\\S+@\\S+\", \" \", t)                # remove emails\n",
    "    t = re.sub(r\"[^a-z0-9\\s]\", \" \", t)            # remove punctuation\n",
    "    t = re.sub(r\"\\b\\d+\\b\", \" \", t)                # remove standalone digits\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()            # collapse spaces\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c2d42d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text: str) -> str:\n",
    "    tokens = text.split()\n",
    "    kept = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n",
    "    return \" \".join(kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad63c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_confusion_matrix(cm, labels, png_path, title=\"Confusion matrix\"):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(png_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b3bc3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -------- main --------\n",
    "def main():\n",
    "    # load dataset\n",
    "    if not os.path.exists(INPUT_PATH):\n",
    "        raise FileNotFoundError(f\"Input file not found: {INPUT_PATH}\")\n",
    "    print(\"Loading:\", INPUT_PATH)\n",
    "    df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "    # detect likely columns\n",
    "    text_col = None\n",
    "    label_col = None\n",
    "    for c in [\"Data\",\"Text\",\"data\",\"text\",\"Content\",\"content\"]:\n",
    "        if c in df.columns:\n",
    "            text_col = c\n",
    "            break\n",
    "    for c in [\"Labels\",\"Label\",\"labels\",\"label\",\"Category\",\"category\"]:\n",
    "        if c in df.columns:\n",
    "            label_col = c\n",
    "            break\n",
    "    if text_col is None or label_col is None:\n",
    "        if len(df.columns) >= 2:\n",
    "            text_col, label_col = df.columns[0], df.columns[1]\n",
    "        else:\n",
    "            raise ValueError(\"Couldn't auto-detect text/label columns in CSV. Ensure it has two columns.\")\n",
    "\n",
    "    df = df[[text_col, label_col]].rename(columns={text_col: \"Data\", label_col: \"Labels\"})\n",
    "    df = df.dropna(subset=[\"Data\", \"Labels\"]).reset_index(drop=True)\n",
    "    print(f\"Rows after dropna: {len(df)}\")\n",
    "    print(\"Label distribution (top 10):\\n\", df[\"Labels\"].value_counts().head(10).to_string())\n",
    "\n",
    "    # Preprocess text (clean + remove stopwords)\n",
    "    print(\"Cleaning text (lowercase, remove urls/emails/punct, drop stopwords)...\")\n",
    "    df[\"clean\"] = df[\"Data\"].astype(str).apply(clean_text).apply(remove_stopwords)\n",
    "    df[\"clean_len\"] = df[\"clean\"].apply(lambda t: len(t.split()))\n",
    "\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[\"Labels\"])\n",
    "    classes = list(le.classes_)\n",
    "    print(\"Classes detected:\", classes)\n",
    "\n",
    "    # Train-test split (stratified)\n",
    "    X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "        df[\"clean\"], y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    print(\"Train size:\", len(X_train_text), \"Test size:\", len(X_test_text))\n",
    "\n",
    "    # Vectorize: fit TF-IDF on train only\n",
    "    print(\"Fitting TF-IDF on training data...\")\n",
    "    vectorizer = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=NGRAM_RANGE, min_df=MIN_DF, sublinear_tf=True)\n",
    "    X_train = vectorizer.fit_transform(X_train_text)\n",
    "    X_test = vectorizer.transform(X_test_text)\n",
    "    print(\"TF-IDF shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "    # Save vectorizer\n",
    "    vec_path = os.path.join(OUTPUT_FOLDER, \"tfidf_vectorizer.joblib\")\n",
    "    joblib.dump(vectorizer, vec_path)\n",
    "    joblib.dump(le, os.path.join(OUTPUT_FOLDER, \"label_encoder.joblib\"))\n",
    "    print(\"Saved vectorizer and label encoder to output folder.\")\n",
    "\n",
    "    # Train Multinomial Naive Bayes\n",
    "    print(\"Training MultinomialNB...\")\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test\n",
    "    y_pred = nb.predict(X_test)\n",
    "    y_prob = nb.predict_proba(X_test) if hasattr(nb, \"predict_proba\") else None\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    prec_weight, rec_weight, f1_weight, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    print(f\"\\nTest Accuracy: {acc:.4f}\")\n",
    "    print(f\"Macro F1: {f1_macro:.4f} | Weighted F1: {f1_weight:.4f}\")\n",
    "\n",
    "    # Classification report & confusion matrix\n",
    "    report = classification_report(y_test, y_pred, target_names=classes, digits=4)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Save artifacts\n",
    "    with open(os.path.join(OUTPUT_FOLDER, \"nb_classification_report.txt\"), \"w\") as f:\n",
    "        f.write(\"Test Accuracy: {:.6f}\\n\\n\".format(acc))\n",
    "        f.write(report)\n",
    "    pd.DataFrame(cm, index=classes, columns=classes).to_csv(os.path.join(OUTPUT_FOLDER, \"nb_confusion_matrix.csv\"))\n",
    "    save_confusion_matrix(cm, classes, os.path.join(OUTPUT_FOLDER, \"nb_confusion_matrix.png\"))\n",
    "\n",
    "    # Save model & predictions\n",
    "    joblib.dump(nb, os.path.join(OUTPUT_FOLDER, \"nb_model.joblib\"))\n",
    "    # Build predictions dataframe aligned to test split\n",
    "    test_indices = X_test_text.index if hasattr(X_test_text, \"index\") else None\n",
    "    preds_df = pd.DataFrame({\n",
    "        \"text\": X_test_text.values,\n",
    "        \"true_label\": le.inverse_transform(y_test),\n",
    "        \"pred_label\": le.inverse_transform(y_pred),\n",
    "        \"pred_confidence\": (y_prob.max(axis=1) if y_prob is not None else None)\n",
    "    })\n",
    "    # The above \"text\" may be an ndarray of strings; ensure correct alignment using iloc on dataframe\n",
    "    # Let's get indices used in the split to be safe:\n",
    "    # We recreate by mapping values (not perfect if duplicates), but better approach is using .iloc indexes:\n",
    "    # Simpler: re-run split with return of indices - but to avoid overcomplicating, save predictions by re-applying vectorizer to original X_test_text\n",
    "    # Save final preds using X_test_text series\n",
    "    preds_df = pd.DataFrame({\n",
    "        \"text\": X_test_text.reset_index(drop=True),\n",
    "        \"true_label\": le.inverse_transform(y_test),\n",
    "        \"pred_label\": le.inverse_transform(y_pred),\n",
    "        \"pred_confidence\": (y_prob.max(axis=1) if y_prob is not None else None)\n",
    "    })\n",
    "    preds_df.to_csv(os.path.join(OUTPUT_FOLDER, \"nb_test_predictions.csv\"), index=False)\n",
    "\n",
    "    # Summary JSON\n",
    "    summary = {\n",
    "        \"n_documents\": int(len(df)),\n",
    "        \"n_classes\": int(len(classes)),\n",
    "        \"classes\": classes,\n",
    "        \"test_size\": int(len(X_test_text)),\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision_macro\": float(prec_macro),\n",
    "        \"recall_macro\": float(rec_macro),\n",
    "        \"f1_macro\": float(f1_macro),\n",
    "        \"precision_weighted\": float(prec_weight),\n",
    "        \"recall_weighted\": float(rec_weight),\n",
    "        \"f1_weighted\": float(f1_weight),\n",
    "    }\n",
    "    with open(os.path.join(OUTPUT_FOLDER, \"nb_summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\nSaved outputs to:\", OUTPUT_FOLDER)\n",
    "    print(\" - nb_model.joblib\")\n",
    "    print(\" - tfidf_vectorizer.joblib\")\n",
    "    print(\" - nb_classification_report.txt\")\n",
    "    print(\" - nb_confusion_matrix.csv/png\")\n",
    "    print(\" - nb_test_predictions.csv\")\n",
    "    print(\" - nb_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa19b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
