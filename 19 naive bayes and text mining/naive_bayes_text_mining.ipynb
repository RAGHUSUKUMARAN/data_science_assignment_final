{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_bayes_text_mining.py\n",
    "\"\"\"\n",
    "Text classification using Multinomial Naive Bayes + sentiment analysis (VADER)\n",
    "Adapt this INPUT_PATH if needed.\n",
    "Saves outputs to the same folder as INPUT_PATH.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a98c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment (VADER)\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cbca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Config - change this path if needed\n",
    "# -----------------------\n",
    "INPUT_PATH = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\19 naive bayes and text mining\\blogs.csv\"\n",
    "OUTPUT_FOLDER = os.path.dirname(INPUT_PATH)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84937e22",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# TF-IDF / model settings\n",
    "MAX_FEATURES = 5000\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "GRID = {\n",
    "    # small grid to tune key hyperparameters quickly\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"clf__alpha\": [0.1, 0.5, 1.0]\n",
    "}\n",
    "CV = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5222fc4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9610423d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text: str):\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9f3e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_confusion_matrix(cm, labels, path_png, title=\"Confusion matrix\"):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_png)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Load & preprocess\n",
    "# -----------------------\n",
    "print(\"Loading:\", INPUT_PATH)\n",
    "df = pd.read_csv(INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58476b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect columns (flexible)\n",
    "text_col = None\n",
    "label_col = None\n",
    "for c in [\"Data\",\"Text\",\"data\",\"text\",\"Content\"]:\n",
    "    if c in df.columns:\n",
    "        text_col = c\n",
    "        break\n",
    "for c in [\"Labels\",\"Label\",\"labels\",\"label\",\"Category\",\"category\"]:\n",
    "    if c in df.columns:\n",
    "        label_col = c\n",
    "        break\n",
    "if text_col is None or label_col is None:\n",
    "    if len(df.columns) >= 2:\n",
    "        text_col, label_col = df.columns[0], df.columns[1]\n",
    "    else:\n",
    "        raise ValueError(\"Could not find text/label columns in CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8749d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[text_col, label_col]].rename(columns={text_col: \"Data\", label_col: \"Labels\"})\n",
    "df = df.dropna(subset=[\"Data\", \"Labels\"]).reset_index(drop=True)\n",
    "print(\"Rows after dropna:\", len(df))\n",
    "print(\"Labels distribution:\\n\", df[\"Labels\"].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "df[\"clean_text\"] = df[\"Data\"].astype(str).apply(clean_text)\n",
    "df[\"clean_text_nostop\"] = df[\"clean_text\"].apply(remove_stopwords)\n",
    "df[\"clean_len_words\"] = df[\"clean_text_nostop\"].apply(lambda t: len(t.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed CSV\n",
    "processed_csv = os.path.join(OUTPUT_FOLDER, \"blogs_processed_naivebayes.csv\")\n",
    "df.to_csv(processed_csv, index=False)\n",
    "print(\"Saved processed CSV:\", processed_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# TF-IDF + train/test\n",
    "# -----------------------\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=(1,2), min_df=2, sublinear_tf=True)\n",
    "X = vectorizer.fit_transform(df[\"clean_text_nostop\"].fillna(\"\"))\n",
    "joblib.dump(vectorizer, os.path.join(OUTPUT_FOLDER, \"tfidf_vectorizer.joblib\"))\n",
    "print(\"TF-IDF shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"Labels\"])\n",
    "joblib.dump(le, os.path.join(OUTPUT_FOLDER, \"label_encoder.joblib\"))\n",
    "print(\"Classes:\", list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "print(\"Train/test sizes:\", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6517df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Baseline MultinomialNB\n",
    "# -----------------------\n",
    "print(\"\\nTraining baseline MultinomialNB...\")\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "y_prob = nb.predict_proba(X_test) if hasattr(nb, \"predict_proba\") else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df05dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "print(f\"Baseline accuracy: {acc:.4f}, f1_macro: {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline model\n",
    "joblib.dump(nb, os.path.join(OUTPUT_FOLDER, \"nb_baseline.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline metrics & reports\n",
    "base_report = classification_report(y_test, y_pred, target_names=le.classes_, digits=4)\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"baseline_classification_report.txt\"), \"w\") as f:\n",
    "    f.write(base_report)\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred), index=le.classes_, columns=le.classes_).to_csv(os.path.join(OUTPUT_FOLDER, \"baseline_confusion_matrix.csv\"))\n",
    "save_confusion_matrix(confusion_matrix(y_test, y_pred), le.classes_, os.path.join(OUTPUT_FOLDER, \"baseline_confusion_matrix.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9840f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline predictions\n",
    "pred_df = pd.DataFrame({\n",
    "    \"text\": df.loc[X_test.indices if hasattr(X_test, 'indices') else X_test.tolist(), \"Data\"].values if False else df.iloc[X_test.nonzero()[0]][\"Data\"].values,  # placeholder not used\n",
    "})\n",
    "# Better approach: map test indices\n",
    "test_idx = X_test.nonzero()[0] if hasattr(X_test, \"nonzero\") else None\n",
    "# We'll use index-based split to save predictions accurately:\n",
    "_, X_test_idx = train_test_split(df.index, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"true_label\": le.inverse_transform(y_test),\n",
    "    \"pred_label\": le.inverse_transform(y_pred),\n",
    "    \"pred_confidence\": y_prob.max(axis=1) if y_prob is not None else None,\n",
    "    \"text\": df.loc[X_test_idx, \"Data\"].values\n",
    "})\n",
    "predictions_df.to_csv(os.path.join(OUTPUT_FOLDER, \"baseline_predictions.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Quick GridSearch (pipeline) to tune alpha + ngram_range\n",
    "# -----------------------\n",
    "print(\"\\nStarting small GridSearch over alpha / ngram_range...\")\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=MAX_FEATURES, min_df=2, sublinear_tf=True)),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "grid = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"clf__alpha\": [0.1, 0.5, 1.0]\n",
    "}\n",
    "gs = GridSearchCV(pipeline, grid, cv=CV, n_jobs=-1, verbose=1, scoring=\"accuracy\")\n",
    "# Fit on raw cleaned text (pipeline will vectorize)\n",
    "gs.fit(df.loc[:, \"clean_text_nostop\"], y)\n",
    "print(\"GridSearch best:\", gs.best_params_, \"best_score:\", gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec53543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best estimator on held-out test set\n",
    "best_model = gs.best_estimator_\n",
    "y_pred_gs = best_model.predict(df.loc[X_test_idx, \"clean_text_nostop\"])\n",
    "acc_gs = accuracy_score(y_test, y_pred_gs)\n",
    "print(f\"Tuned model accuracy on test set: {acc_gs:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d16aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuned pipeline\n",
    "joblib.dump(best_model, os.path.join(OUTPUT_FOLDER, \"nb_tuned_pipeline.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuned reports\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"tuned_classification_report.txt\"), \"w\") as f:\n",
    "    f.write(classification_report(y_test, y_pred_gs, target_names=le.classes_, digits=4))\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred_gs), index=le.classes_, columns=le.classes_).to_csv(os.path.join(OUTPUT_FOLDER, \"tuned_confusion_matrix.csv\"))\n",
    "save_confusion_matrix(confusion_matrix(y_test, y_pred_gs), le.classes_, os.path.join(OUTPUT_FOLDER, \"tuned_confusion_matrix.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Sentiment analysis using VADER\n",
    "# -----------------------\n",
    "print(\"\\nRunning VADER sentiment analysis...\")\n",
    "try:\n",
    "    nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
    "except LookupError:\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a77071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sentiment scores\n",
    "sent_scores = df[\"Data\"].astype(str).apply(lambda t: sia.polarity_scores(t)[\"compound\"])\n",
    "def sentiment_label(c):\n",
    "    if c >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif c <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "df[\"sentiment_score\"] = sent_scores\n",
    "df[\"sentiment_label\"] = df[\"sentiment_score\"].apply(sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV with sentiment + predictions (merge predictions_df on text)\n",
    "# Attach baseline predictions where possible by index:\n",
    "# We already saved predictions_df for the test subset; let's save overall sentiment + labels for full data.\n",
    "df.to_csv(os.path.join(OUTPUT_FOLDER, \"blogs_with_sentiment.csv\"), index=False)\n",
    "print(\"Saved sentiment-annotated CSV to:\", os.path.join(OUTPUT_FOLDER, \"blogs_with_sentiment.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54543cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Summary JSON for assignment\n",
    "# -----------------------\n",
    "summary = {\n",
    "    \"n_documents\": int(len(df)),\n",
    "    \"n_classes\": int(len(le.classes_)),\n",
    "    \"classes\": list(le.classes_),\n",
    "    \"baseline_accuracy\": float(acc),\n",
    "    \"tuned_grid_best\": gs.best_params_,\n",
    "    \"tuned_cv_score\": float(gs.best_score_),\n",
    "    \"tuned_test_accuracy\": float(acc_gs)\n",
    "}\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"nb_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAll done. Outputs saved to:\", OUTPUT_FOLDER)\n",
    "print(\"Key files:\")\n",
    "print(\" - baseline_classification_report.txt\")\n",
    "print(\" - baseline_confusion_matrix.csv/png\")\n",
    "print(\" - baseline_predictions.csv\")\n",
    "print(\" - nb_baseline.joblib\")\n",
    "print(\" - nb_tuned_pipeline.joblib (GridSearch best)\")\n",
    "print(\" - tuned_classification_report.txt\")\n",
    "print(\" - blogs_with_sentiment.csv\")\n",
    "print(\" - nb_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
