{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce01f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4_nlp.py\n",
    "\"\"\"\n",
    "Evaluates Naive Bayes Text Classifier on blogs.csv and performs evaluation artifacts.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d116e7f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -------- File Paths --------\n",
    "INPUT_PATH = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\19 naive bayes and text mining\\blogs.csv\"\n",
    "OUTPUT_PATH = os.path.dirname(INPUT_PATH)\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090308ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -------- Preprocessing --------\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999ab3b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "stopwords = set(ENGLISH_STOP_WORDS)\n",
    "def remove_stopwords(t: str) -> str:\n",
    "    return \" \".join([w for w in t.split() if w not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_PATH)\n",
    "text_col = \"Data\" if \"Data\" in df.columns else df.columns[0]\n",
    "label_col = \"Labels\" if \"Labels\" in df.columns else df.columns[1]\n",
    "df = df.dropna(subset=[text_col, label_col]).reset_index(drop=True)\n",
    "df[\"clean\"] = df[text_col].apply(clean_text).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d792e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[label_col])\n",
    "classes = list(le.classes_)  # <— used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b897b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"clean\"], y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adaaaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Baseline NB --------\n",
    "vec = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2, sublinear_tf=True)\n",
    "X_train_vec = vec.fit_transform(X_train)\n",
    "X_test_vec  = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22273ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train_vec, y_train)\n",
    "y_pred = nb.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb67364",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "prec_weight, rec_weight, f1_weight, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\", zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8fec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline Accuracy: {acc:.4f}\")\n",
    "print(f\"Macro F1: {f1_macro:.4f} | Weighted F1: {f1_weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d628551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- GridSearch Tuning --------\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=5000, min_df=2, sublinear_tf=True)),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"clf__alpha\": [0.1, 0.5, 1.0]\n",
    "}\n",
    "gs = GridSearchCV(pipeline, params, cv=4, n_jobs=-1, scoring=\"accuracy\")\n",
    "gs.fit(X_train, y_train)\n",
    "y_pred_gs = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_gs = accuracy_score(y_test, y_pred_gs)\n",
    "f1_macro_gs = precision_recall_fscore_support(y_test, y_pred_gs, average=\"macro\", zero_division=0)[2]\n",
    "f1_w_gs     = precision_recall_fscore_support(y_test, y_pred_gs, average=\"weighted\", zero_division=0)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3428e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tuned Accuracy: {acc_gs:.4f} | Macro F1: {f1_macro_gs:.4f}\")\n",
    "print(\"Best Params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Save Results --------\n",
    "summary = {\n",
    "    \"baseline\": {\"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_weighted\": f1_weight},\n",
    "    \"tuned\":    {\"accuracy\": acc_gs, \"f1_macro\": f1_macro_gs, \"f1_weighted\": f1_w_gs, \"best_params\": gs.best_params_}\n",
    "}\n",
    "with open(os.path.join(OUTPUT_PATH, \"nb_results.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"Results saved to nb_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95776731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Visualization and Comparison\n",
    "# ----------------------------\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d551b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix from the **tuned** predictions (use baseline if you prefer)\n",
    "cm = confusion_matrix(y_test, y_pred_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c72d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Confusion Matrix Plot ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix — Naive Bayes\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(classes)), classes, rotation=90)\n",
    "plt.yticks(range(len(classes)), classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "confusion_path = os.path.join(OUTPUT_PATH, \"nb_confusion_matrix.png\")\n",
    "plt.savefig(confusion_path, dpi=300)\n",
    "plt.close()\n",
    "print(f\"Saved confusion matrix plot to: {confusion_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save CSV version of the confusion matrix (handy for the appendix)\n",
    "pd.DataFrame(cm, index=classes, columns=classes).to_csv(\n",
    "    os.path.join(OUTPUT_PATH, \"nb_confusion_matrix.csv\"), index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eeb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baseline vs Tuned Comparison ---\n",
    "baseline_metrics = {\"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_weighted\": f1_weight}\n",
    "tuned_metrics    = {\"accuracy\": acc_gs, \"f1_macro\": f1_macro_gs, \"f1_weighted\": f1_w_gs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00387bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.DataFrame([\n",
    "    {\"model\": \"baseline\", **baseline_metrics},\n",
    "    {\"model\": \"tuned\",    **tuned_metrics}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_path = os.path.join(OUTPUT_PATH, \"baseline_vs_tuned_metrics.csv\")\n",
    "comp_df.to_csv(comparison_path, index=False)\n",
    "print(f\"Saved metric comparison to: {comparison_path}\")\n",
    "print(\"\\nComparison Table:\\n\", comp_df)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
