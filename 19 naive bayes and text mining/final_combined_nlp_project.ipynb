{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce01f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4_nlp.py\n",
    "\"\"\"\n",
    "Evaluates Naive Bayes Text Classifier on blogs.csv and performs evaluation artifacts.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d116e7f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -------- File Paths --------\n",
    "INPUT_PATH = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\19 naive bayes and text mining\\blogs.csv\"\n",
    "OUTPUT_PATH = os.path.dirname(INPUT_PATH)\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090308ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -------- Preprocessing --------\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999ab3b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "stopwords = set(ENGLISH_STOP_WORDS)\n",
    "def remove_stopwords(t: str) -> str:\n",
    "    return \" \".join([w for w in t.split() if w not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_PATH)\n",
    "text_col = \"Data\" if \"Data\" in df.columns else df.columns[0]\n",
    "label_col = \"Labels\" if \"Labels\" in df.columns else df.columns[1]\n",
    "df = df.dropna(subset=[text_col, label_col]).reset_index(drop=True)\n",
    "df[\"clean\"] = df[text_col].apply(clean_text).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d792e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[label_col])\n",
    "classes = list(le.classes_)  # <— used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b897b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"clean\"], y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adaaaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Baseline NB --------\n",
    "vec = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2, sublinear_tf=True)\n",
    "X_train_vec = vec.fit_transform(X_train)\n",
    "X_test_vec  = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22273ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train_vec, y_train)\n",
    "y_pred = nb.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb67364",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "prec_weight, rec_weight, f1_weight, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\", zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8fec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline Accuracy: {acc:.4f}\")\n",
    "print(f\"Macro F1: {f1_macro:.4f} | Weighted F1: {f1_weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d628551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- GridSearch Tuning --------\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=5000, min_df=2, sublinear_tf=True)),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"clf__alpha\": [0.1, 0.5, 1.0]\n",
    "}\n",
    "gs = GridSearchCV(pipeline, params, cv=4, n_jobs=-1, scoring=\"accuracy\")\n",
    "gs.fit(X_train, y_train)\n",
    "y_pred_gs = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_gs = accuracy_score(y_test, y_pred_gs)\n",
    "f1_macro_gs = precision_recall_fscore_support(y_test, y_pred_gs, average=\"macro\", zero_division=0)[2]\n",
    "f1_w_gs     = precision_recall_fscore_support(y_test, y_pred_gs, average=\"weighted\", zero_division=0)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3428e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tuned Accuracy: {acc_gs:.4f} | Macro F1: {f1_macro_gs:.4f}\")\n",
    "print(\"Best Params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Save Results --------\n",
    "summary = {\n",
    "    \"baseline\": {\"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_weighted\": f1_weight},\n",
    "    \"tuned\":    {\"accuracy\": acc_gs, \"f1_macro\": f1_macro_gs, \"f1_weighted\": f1_w_gs, \"best_params\": gs.best_params_}\n",
    "}\n",
    "with open(os.path.join(OUTPUT_PATH, \"nb_results.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"Results saved to nb_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95776731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Visualization and Comparison\n",
    "# ----------------------------\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d551b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix from the **tuned** predictions (use baseline if you prefer)\n",
    "cm = confusion_matrix(y_test, y_pred_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c72d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Confusion Matrix Plot ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix — Naive Bayes\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(classes)), classes, rotation=90)\n",
    "plt.yticks(range(len(classes)), classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "confusion_path = os.path.join(OUTPUT_PATH, \"nb_confusion_matrix.png\")\n",
    "plt.savefig(confusion_path, dpi=300)\n",
    "plt.close()\n",
    "print(f\"Saved confusion matrix plot to: {confusion_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save CSV version of the confusion matrix (handy for the appendix)\n",
    "pd.DataFrame(cm, index=classes, columns=classes).to_csv(\n",
    "    os.path.join(OUTPUT_PATH, \"nb_confusion_matrix.csv\"), index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eeb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baseline vs Tuned Comparison ---\n",
    "baseline_metrics = {\"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_weighted\": f1_weight}\n",
    "tuned_metrics    = {\"accuracy\": acc_gs, \"f1_macro\": f1_macro_gs, \"f1_weighted\": f1_w_gs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00387bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.DataFrame([\n",
    "    {\"model\": \"baseline\", **baseline_metrics},\n",
    "    {\"model\": \"tuned\",    **tuned_metrics}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_path = os.path.join(OUTPUT_PATH, \"baseline_vs_tuned_metrics.csv\")\n",
    "comp_df.to_csv(comparison_path, index=False)\n",
    "print(f\"Saved metric comparison to: {comparison_path}\")\n",
    "print(\"\\nComparison Table:\\n\", comp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d641a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Confusion matrix — Naive Bayes\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(classes)), classes, rotation=90)\n",
    "plt.yticks(range(len(classes)), classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"nb_confusion_matrix.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f248b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_metrics and tuned_metrics are dicts like the 'summary' above\n",
    "comp_df = pd.DataFrame([\n",
    "    {\"model\":\"baseline\", **baseline_metrics},\n",
    "    {\"model\":\"tuned\", **tuned_metrics}\n",
    "])\n",
    "comp_df.to_csv(\"baseline_vs_tuned_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_bayes_text_mining.py\n",
    "\"\"\"\n",
    "Text classification using Multinomial Naive Bayes + sentiment analysis (VADER)\n",
    "Adapt this INPUT_PATH if needed.\n",
    "Saves outputs to the same folder as INPUT_PATH.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a98c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment (VADER)\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cbca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Config - change this path if needed\n",
    "# -----------------------\n",
    "INPUT_PATH = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\19 naive bayes and text mining\\blogs.csv\"\n",
    "OUTPUT_FOLDER = os.path.dirname(INPUT_PATH)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84937e22",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# TF-IDF / model settings\n",
    "MAX_FEATURES = 5000\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "GRID = {\n",
    "    # small grid to tune key hyperparameters quickly\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"clf__alpha\": [0.1, 0.5, 1.0]\n",
    "}\n",
    "CV = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5222fc4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9610423d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text: str):\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9f3e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_confusion_matrix(cm, labels, path_png, title=\"Confusion matrix\"):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_png)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Load & preprocess\n",
    "# -----------------------\n",
    "print(\"Loading:\", INPUT_PATH)\n",
    "df = pd.read_csv(INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58476b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect columns (flexible)\n",
    "text_col = None\n",
    "label_col = None\n",
    "for c in [\"Data\",\"Text\",\"data\",\"text\",\"Content\"]:\n",
    "    if c in df.columns:\n",
    "        text_col = c\n",
    "        break\n",
    "for c in [\"Labels\",\"Label\",\"labels\",\"label\",\"Category\",\"category\"]:\n",
    "    if c in df.columns:\n",
    "        label_col = c\n",
    "        break\n",
    "if text_col is None or label_col is None:\n",
    "    if len(df.columns) >= 2:\n",
    "        text_col, label_col = df.columns[0], df.columns[1]\n",
    "    else:\n",
    "        raise ValueError(\"Could not find text/label columns in CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8749d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[text_col, label_col]].rename(columns={text_col: \"Data\", label_col: \"Labels\"})\n",
    "df = df.dropna(subset=[\"Data\", \"Labels\"]).reset_index(drop=True)\n",
    "print(\"Rows after dropna:\", len(df))\n",
    "print(\"Labels distribution:\\n\", df[\"Labels\"].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "df[\"clean_text\"] = df[\"Data\"].astype(str).apply(clean_text)\n",
    "df[\"clean_text_nostop\"] = df[\"clean_text\"].apply(remove_stopwords)\n",
    "df[\"clean_len_words\"] = df[\"clean_text_nostop\"].apply(lambda t: len(t.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed CSV\n",
    "processed_csv = os.path.join(OUTPUT_FOLDER, \"blogs_processed_naivebayes.csv\")\n",
    "df.to_csv(processed_csv, index=False)\n",
    "print(\"Saved processed CSV:\", processed_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# TF-IDF + train/test\n",
    "# -----------------------\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=(1,2), min_df=2, sublinear_tf=True)\n",
    "X = vectorizer.fit_transform(df[\"clean_text_nostop\"].fillna(\"\"))\n",
    "joblib.dump(vectorizer, os.path.join(OUTPUT_FOLDER, \"tfidf_vectorizer.joblib\"))\n",
    "print(\"TF-IDF shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"Labels\"])\n",
    "joblib.dump(le, os.path.join(OUTPUT_FOLDER, \"label_encoder.joblib\"))\n",
    "print(\"Classes:\", list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "print(\"Train/test sizes:\", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6517df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Baseline MultinomialNB\n",
    "# -----------------------\n",
    "print(\"\\nTraining baseline MultinomialNB...\")\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "y_prob = nb.predict_proba(X_test) if hasattr(nb, \"predict_proba\") else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df05dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "print(f\"Baseline accuracy: {acc:.4f}, f1_macro: {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline model\n",
    "joblib.dump(nb, os.path.join(OUTPUT_FOLDER, \"nb_baseline.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline metrics & reports\n",
    "base_report = classification_report(y_test, y_pred, target_names=le.classes_, digits=4)\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"baseline_classification_report.txt\"), \"w\") as f:\n",
    "    f.write(base_report)\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred), index=le.classes_, columns=le.classes_).to_csv(os.path.join(OUTPUT_FOLDER, \"baseline_confusion_matrix.csv\"))\n",
    "save_confusion_matrix(confusion_matrix(y_test, y_pred), le.classes_, os.path.join(OUTPUT_FOLDER, \"baseline_confusion_matrix.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9840f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline predictions\n",
    "pred_df = pd.DataFrame({\n",
    "    \"text\": df.loc[X_test.indices if hasattr(X_test, 'indices') else X_test.tolist(), \"Data\"].values if False else df.iloc[X_test.nonzero()[0]][\"Data\"].values,  # placeholder not used\n",
    "})\n",
    "# Better approach: map test indices\n",
    "test_idx = X_test.nonzero()[0] if hasattr(X_test, \"nonzero\") else None\n",
    "# We'll use index-based split to save predictions accurately:\n",
    "_, X_test_idx = train_test_split(df.index, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"true_label\": le.inverse_transform(y_test),\n",
    "    \"pred_label\": le.inverse_transform(y_pred),\n",
    "    \"pred_confidence\": y_prob.max(axis=1) if y_prob is not None else None,\n",
    "    \"text\": df.loc[X_test_idx, \"Data\"].values\n",
    "})\n",
    "predictions_df.to_csv(os.path.join(OUTPUT_FOLDER, \"baseline_predictions.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Quick GridSearch (pipeline) to tune alpha + ngram_range\n",
    "# -----------------------\n",
    "print(\"\\nStarting small GridSearch over alpha / ngram_range...\")\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=MAX_FEATURES, min_df=2, sublinear_tf=True)),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "grid = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"clf__alpha\": [0.1, 0.5, 1.0]\n",
    "}\n",
    "gs = GridSearchCV(pipeline, grid, cv=CV, n_jobs=-1, verbose=1, scoring=\"accuracy\")\n",
    "# Fit on raw cleaned text (pipeline will vectorize)\n",
    "gs.fit(df.loc[:, \"clean_text_nostop\"], y)\n",
    "print(\"GridSearch best:\", gs.best_params_, \"best_score:\", gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec53543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best estimator on held-out test set\n",
    "best_model = gs.best_estimator_\n",
    "y_pred_gs = best_model.predict(df.loc[X_test_idx, \"clean_text_nostop\"])\n",
    "acc_gs = accuracy_score(y_test, y_pred_gs)\n",
    "print(f\"Tuned model accuracy on test set: {acc_gs:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d16aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuned pipeline\n",
    "joblib.dump(best_model, os.path.join(OUTPUT_FOLDER, \"nb_tuned_pipeline.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuned reports\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"tuned_classification_report.txt\"), \"w\") as f:\n",
    "    f.write(classification_report(y_test, y_pred_gs, target_names=le.classes_, digits=4))\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred_gs), index=le.classes_, columns=le.classes_).to_csv(os.path.join(OUTPUT_FOLDER, \"tuned_confusion_matrix.csv\"))\n",
    "save_confusion_matrix(confusion_matrix(y_test, y_pred_gs), le.classes_, os.path.join(OUTPUT_FOLDER, \"tuned_confusion_matrix.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Sentiment analysis using VADER\n",
    "# -----------------------\n",
    "print(\"\\nRunning VADER sentiment analysis...\")\n",
    "try:\n",
    "    nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
    "except LookupError:\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a77071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sentiment scores\n",
    "sent_scores = df[\"Data\"].astype(str).apply(lambda t: sia.polarity_scores(t)[\"compound\"])\n",
    "def sentiment_label(c):\n",
    "    if c >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif c <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "df[\"sentiment_score\"] = sent_scores\n",
    "df[\"sentiment_label\"] = df[\"sentiment_score\"].apply(sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV with sentiment + predictions (merge predictions_df on text)\n",
    "# Attach baseline predictions where possible by index:\n",
    "# We already saved predictions_df for the test subset; let's save overall sentiment + labels for full data.\n",
    "df.to_csv(os.path.join(OUTPUT_FOLDER, \"blogs_with_sentiment.csv\"), index=False)\n",
    "print(\"Saved sentiment-annotated CSV to:\", os.path.join(OUTPUT_FOLDER, \"blogs_with_sentiment.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54543cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Summary JSON for assignment\n",
    "# -----------------------\n",
    "summary = {\n",
    "    \"n_documents\": int(len(df)),\n",
    "    \"n_classes\": int(len(le.classes_)),\n",
    "    \"classes\": list(le.classes_),\n",
    "    \"baseline_accuracy\": float(acc),\n",
    "    \"tuned_grid_best\": gs.best_params_,\n",
    "    \"tuned_cv_score\": float(gs.best_score_),\n",
    "    \"tuned_test_accuracy\": float(acc_gs)\n",
    "}\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"nb_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAll done. Outputs saved to:\", OUTPUT_FOLDER)\n",
    "print(\"Key files:\")\n",
    "print(\" - baseline_classification_report.txt\")\n",
    "print(\" - baseline_confusion_matrix.csv/png\")\n",
    "print(\" - baseline_predictions.csv\")\n",
    "print(\" - nb_baseline.joblib\")\n",
    "print(\" - nb_tuned_pipeline.joblib (GridSearch best)\")\n",
    "print(\" - tuned_classification_report.txt\")\n",
    "print(\" - blogs_with_sentiment.csv\")\n",
    "print(\" - nb_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d317eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_train.py\n",
    "\"\"\"\n",
    "Naive Bayes text classifier (Task 2)\n",
    "- Change INPUT_PATH if needed.\n",
    "- Saves outputs (model, vectorizer, reports) to the same folder as INPUT_PATH.\n",
    "Requirements:\n",
    "    pip install numpy pandas scikit-learn matplotlib joblib\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7adb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f096761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f37df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- CONFIG --------\n",
    "INPUT_PATH = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\19 naive bayes and text mining\\blogs.csv\"\n",
    "OUTPUT_FOLDER = os.path.dirname(INPUT_PATH)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e99898",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "MAX_FEATURES = 5000   # change if you want fewer/more features\n",
    "NGRAM_RANGE = (1,2)   # unigrams + bigrams\n",
    "MIN_DF = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d580e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -------- helpers --------\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text.lower()\n",
    "    t = re.sub(r\"http\\S+|www\\.\\S+\", \" \", t)        # remove urls\n",
    "    t = re.sub(r\"\\S+@\\S+\", \" \", t)                # remove emails\n",
    "    t = re.sub(r\"[^a-z0-9\\s]\", \" \", t)            # remove punctuation\n",
    "    t = re.sub(r\"\\b\\d+\\b\", \" \", t)                # remove standalone digits\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()            # collapse spaces\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c2d42d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text: str) -> str:\n",
    "    tokens = text.split()\n",
    "    kept = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n",
    "    return \" \".join(kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad63c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_confusion_matrix(cm, labels, png_path, title=\"Confusion matrix\"):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(png_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b3bc3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -------- main --------\n",
    "def main():\n",
    "    # load dataset\n",
    "    if not os.path.exists(INPUT_PATH):\n",
    "        raise FileNotFoundError(f\"Input file not found: {INPUT_PATH}\")\n",
    "    print(\"Loading:\", INPUT_PATH)\n",
    "    df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "    # detect likely columns\n",
    "    text_col = None\n",
    "    label_col = None\n",
    "    for c in [\"Data\",\"Text\",\"data\",\"text\",\"Content\",\"content\"]:\n",
    "        if c in df.columns:\n",
    "            text_col = c\n",
    "            break\n",
    "    for c in [\"Labels\",\"Label\",\"labels\",\"label\",\"Category\",\"category\"]:\n",
    "        if c in df.columns:\n",
    "            label_col = c\n",
    "            break\n",
    "    if text_col is None or label_col is None:\n",
    "        if len(df.columns) >= 2:\n",
    "            text_col, label_col = df.columns[0], df.columns[1]\n",
    "        else:\n",
    "            raise ValueError(\"Couldn't auto-detect text/label columns in CSV. Ensure it has two columns.\")\n",
    "\n",
    "    df = df[[text_col, label_col]].rename(columns={text_col: \"Data\", label_col: \"Labels\"})\n",
    "    df = df.dropna(subset=[\"Data\", \"Labels\"]).reset_index(drop=True)\n",
    "    print(f\"Rows after dropna: {len(df)}\")\n",
    "    print(\"Label distribution (top 10):\\n\", df[\"Labels\"].value_counts().head(10).to_string())\n",
    "\n",
    "    # Preprocess text (clean + remove stopwords)\n",
    "    print(\"Cleaning text (lowercase, remove urls/emails/punct, drop stopwords)...\")\n",
    "    df[\"clean\"] = df[\"Data\"].astype(str).apply(clean_text).apply(remove_stopwords)\n",
    "    df[\"clean_len\"] = df[\"clean\"].apply(lambda t: len(t.split()))\n",
    "\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df[\"Labels\"])\n",
    "    classes = list(le.classes_)\n",
    "    print(\"Classes detected:\", classes)\n",
    "\n",
    "    # Train-test split (stratified)\n",
    "    X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "        df[\"clean\"], y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    print(\"Train size:\", len(X_train_text), \"Test size:\", len(X_test_text))\n",
    "\n",
    "    # Vectorize: fit TF-IDF on train only\n",
    "    print(\"Fitting TF-IDF on training data...\")\n",
    "    vectorizer = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=NGRAM_RANGE, min_df=MIN_DF, sublinear_tf=True)\n",
    "    X_train = vectorizer.fit_transform(X_train_text)\n",
    "    X_test = vectorizer.transform(X_test_text)\n",
    "    print(\"TF-IDF shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "    # Save vectorizer\n",
    "    vec_path = os.path.join(OUTPUT_FOLDER, \"tfidf_vectorizer.joblib\")\n",
    "    joblib.dump(vectorizer, vec_path)\n",
    "    joblib.dump(le, os.path.join(OUTPUT_FOLDER, \"label_encoder.joblib\"))\n",
    "    print(\"Saved vectorizer and label encoder to output folder.\")\n",
    "\n",
    "    # Train Multinomial Naive Bayes\n",
    "    print(\"Training MultinomialNB...\")\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test\n",
    "    y_pred = nb.predict(X_test)\n",
    "    y_prob = nb.predict_proba(X_test) if hasattr(nb, \"predict_proba\") else None\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    prec_weight, rec_weight, f1_weight, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    print(f\"\\nTest Accuracy: {acc:.4f}\")\n",
    "    print(f\"Macro F1: {f1_macro:.4f} | Weighted F1: {f1_weight:.4f}\")\n",
    "\n",
    "    # Classification report & confusion matrix\n",
    "    report = classification_report(y_test, y_pred, target_names=classes, digits=4)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Save artifacts\n",
    "    with open(os.path.join(OUTPUT_FOLDER, \"nb_classification_report.txt\"), \"w\") as f:\n",
    "        f.write(\"Test Accuracy: {:.6f}\\n\\n\".format(acc))\n",
    "        f.write(report)\n",
    "    pd.DataFrame(cm, index=classes, columns=classes).to_csv(os.path.join(OUTPUT_FOLDER, \"nb_confusion_matrix.csv\"))\n",
    "    save_confusion_matrix(cm, classes, os.path.join(OUTPUT_FOLDER, \"nb_confusion_matrix.png\"))\n",
    "\n",
    "    # Save model & predictions\n",
    "    joblib.dump(nb, os.path.join(OUTPUT_FOLDER, \"nb_model.joblib\"))\n",
    "    # Build predictions dataframe aligned to test split\n",
    "    test_indices = X_test_text.index if hasattr(X_test_text, \"index\") else None\n",
    "    preds_df = pd.DataFrame({\n",
    "        \"text\": X_test_text.values,\n",
    "        \"true_label\": le.inverse_transform(y_test),\n",
    "        \"pred_label\": le.inverse_transform(y_pred),\n",
    "        \"pred_confidence\": (y_prob.max(axis=1) if y_prob is not None else None)\n",
    "    })\n",
    "    # The above \"text\" may be an ndarray of strings; ensure correct alignment using iloc on dataframe\n",
    "    # Let's get indices used in the split to be safe:\n",
    "    # We recreate by mapping values (not perfect if duplicates), but better approach is using .iloc indexes:\n",
    "    # Simpler: re-run split with return of indices - but to avoid overcomplicating, save predictions by re-applying vectorizer to original X_test_text\n",
    "    # Save final preds using X_test_text series\n",
    "    preds_df = pd.DataFrame({\n",
    "        \"text\": X_test_text.reset_index(drop=True),\n",
    "        \"true_label\": le.inverse_transform(y_test),\n",
    "        \"pred_label\": le.inverse_transform(y_pred),\n",
    "        \"pred_confidence\": (y_prob.max(axis=1) if y_prob is not None else None)\n",
    "    })\n",
    "    preds_df.to_csv(os.path.join(OUTPUT_FOLDER, \"nb_test_predictions.csv\"), index=False)\n",
    "\n",
    "    # Summary JSON\n",
    "    summary = {\n",
    "        \"n_documents\": int(len(df)),\n",
    "        \"n_classes\": int(len(classes)),\n",
    "        \"classes\": classes,\n",
    "        \"test_size\": int(len(X_test_text)),\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision_macro\": float(prec_macro),\n",
    "        \"recall_macro\": float(rec_macro),\n",
    "        \"f1_macro\": float(f1_macro),\n",
    "        \"precision_weighted\": float(prec_weight),\n",
    "        \"recall_weighted\": float(rec_weight),\n",
    "        \"f1_weighted\": float(f1_weight),\n",
    "    }\n",
    "    with open(os.path.join(OUTPUT_FOLDER, \"nb_summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\nSaved outputs to:\", OUTPUT_FOLDER)\n",
    "    print(\" - nb_model.joblib\")\n",
    "    print(\" - tfidf_vectorizer.joblib\")\n",
    "    print(\" - nb_classification_report.txt\")\n",
    "    print(\" - nb_confusion_matrix.csv/png\")\n",
    "    print(\" - nb_test_predictions.csv\")\n",
    "    print(\" - nb_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa19b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe833bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_bayes_text_mining.py\n",
    "\"\"\"\n",
    "Text classification using Multinomial Naive Bayes + sentiment analysis (VADER)\n",
    "Adapt this INPUT_PATH if needed.\n",
    "Saves outputs to the same folder as INPUT_PATH.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc34bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment (VADER)\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Config - change this path if needed\n",
    "# -----------------------\n",
    "INPUT_PATH = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\19 naive bayes and text mining\\blogs.csv\"\n",
    "OUTPUT_FOLDER = os.path.dirname(INPUT_PATH)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c633a53",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# TF-IDF / model settings\n",
    "MAX_FEATURES = 5000\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "GRID = {\n",
    "    # small grid to tune key hyperparameters quickly\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"clf__alpha\": [0.1, 0.5, 1.0]\n",
    "}\n",
    "CV = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5efc0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c36805",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text: str):\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a9e2e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_confusion_matrix(cm, labels, path_png, title=\"Confusion matrix\"):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_png)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e004598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Load & preprocess\n",
    "# -----------------------\n",
    "print(\"Loading:\", INPUT_PATH)\n",
    "df = pd.read_csv(INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edf053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect columns (flexible)\n",
    "text_col = None\n",
    "label_col = None\n",
    "for c in [\"Data\",\"Text\",\"data\",\"text\",\"Content\"]:\n",
    "    if c in df.columns:\n",
    "        text_col = c\n",
    "        break\n",
    "for c in [\"Labels\",\"Label\",\"labels\",\"label\",\"Category\",\"category\"]:\n",
    "    if c in df.columns:\n",
    "        label_col = c\n",
    "        break\n",
    "if text_col is None or label_col is None:\n",
    "    if len(df.columns) >= 2:\n",
    "        text_col, label_col = df.columns[0], df.columns[1]\n",
    "    else:\n",
    "        raise ValueError(\"Could not find text/label columns in CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d2c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[text_col, label_col]].rename(columns={text_col: \"Data\", label_col: \"Labels\"})\n",
    "df = df.dropna(subset=[\"Data\", \"Labels\"]).reset_index(drop=True)\n",
    "print(\"Rows after dropna:\", len(df))\n",
    "print(\"Labels distribution:\\n\", df[\"Labels\"].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3df461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "df[\"clean_text\"] = df[\"Data\"].astype(str).apply(clean_text)\n",
    "df[\"clean_text_nostop\"] = df[\"clean_text\"].apply(remove_stopwords)\n",
    "df[\"clean_len_words\"] = df[\"clean_text_nostop\"].apply(lambda t: len(t.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ad307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed CSV\n",
    "processed_csv = os.path.join(OUTPUT_FOLDER, \"blogs_processed_naivebayes.csv\")\n",
    "df.to_csv(processed_csv, index=False)\n",
    "print(\"Saved processed CSV:\", processed_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0192dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# TF-IDF + train/test\n",
    "# -----------------------\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=(1,2), min_df=2, sublinear_tf=True)\n",
    "X = vectorizer.fit_transform(df[\"clean_text_nostop\"].fillna(\"\"))\n",
    "joblib.dump(vectorizer, os.path.join(OUTPUT_FOLDER, \"tfidf_vectorizer.joblib\"))\n",
    "print(\"TF-IDF shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"Labels\"])\n",
    "joblib.dump(le, os.path.join(OUTPUT_FOLDER, \"label_encoder.joblib\"))\n",
    "print(\"Classes:\", list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "print(\"Train/test sizes:\", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32005e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Baseline MultinomialNB\n",
    "# -----------------------\n",
    "print(\"\\nTraining baseline MultinomialNB...\")\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "y_prob = nb.predict_proba(X_test) if hasattr(nb, \"predict_proba\") else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "print(f\"Baseline accuracy: {acc:.4f}, f1_macro: {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae10bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline model\n",
    "joblib.dump(nb, os.path.join(OUTPUT_FOLDER, \"nb_baseline.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b7e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline metrics & reports\n",
    "base_report = classification_report(y_test, y_pred, target_names=le.classes_, digits=4)\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"baseline_classification_report.txt\"), \"w\") as f:\n",
    "    f.write(base_report)\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred), index=le.classes_, columns=le.classes_).to_csv(os.path.join(OUTPUT_FOLDER, \"baseline_confusion_matrix.csv\"))\n",
    "save_confusion_matrix(confusion_matrix(y_test, y_pred), le.classes_, os.path.join(OUTPUT_FOLDER, \"baseline_confusion_matrix.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37833adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline predictions\n",
    "pred_df = pd.DataFrame({\n",
    "    \"text\": df.loc[X_test.indices if hasattr(X_test, 'indices') else X_test.tolist(), \"Data\"].values if False else df.iloc[X_test.nonzero()[0]][\"Data\"].values,  # placeholder not used\n",
    "})\n",
    "# Better approach: map test indices\n",
    "test_idx = X_test.nonzero()[0] if hasattr(X_test, \"nonzero\") else None\n",
    "# We'll use index-based split to save predictions accurately:\n",
    "_, X_test_idx = train_test_split(df.index, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"true_label\": le.inverse_transform(y_test),\n",
    "    \"pred_label\": le.inverse_transform(y_pred),\n",
    "    \"pred_confidence\": y_prob.max(axis=1) if y_prob is not None else None,\n",
    "    \"text\": df.loc[X_test_idx, \"Data\"].values\n",
    "})\n",
    "predictions_df.to_csv(os.path.join(OUTPUT_FOLDER, \"baseline_predictions.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b1b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Quick GridSearch (pipeline) to tune alpha + ngram_range\n",
    "# -----------------------\n",
    "print(\"\\nStarting small GridSearch over alpha / ngram_range...\")\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=MAX_FEATURES, min_df=2, sublinear_tf=True)),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "grid = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"clf__alpha\": [0.1, 0.5, 1.0]\n",
    "}\n",
    "gs = GridSearchCV(pipeline, grid, cv=CV, n_jobs=-1, verbose=1, scoring=\"accuracy\")\n",
    "# Fit on raw cleaned text (pipeline will vectorize)\n",
    "gs.fit(df.loc[:, \"clean_text_nostop\"], y)\n",
    "print(\"GridSearch best:\", gs.best_params_, \"best_score:\", gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cbacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best estimator on held-out test set\n",
    "best_model = gs.best_estimator_\n",
    "y_pred_gs = best_model.predict(df.loc[X_test_idx, \"clean_text_nostop\"])\n",
    "acc_gs = accuracy_score(y_test, y_pred_gs)\n",
    "print(f\"Tuned model accuracy on test set: {acc_gs:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a9747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuned pipeline\n",
    "joblib.dump(best_model, os.path.join(OUTPUT_FOLDER, \"nb_tuned_pipeline.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuned reports\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"tuned_classification_report.txt\"), \"w\") as f:\n",
    "    f.write(classification_report(y_test, y_pred_gs, target_names=le.classes_, digits=4))\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred_gs), index=le.classes_, columns=le.classes_).to_csv(os.path.join(OUTPUT_FOLDER, \"tuned_confusion_matrix.csv\"))\n",
    "save_confusion_matrix(confusion_matrix(y_test, y_pred_gs), le.classes_, os.path.join(OUTPUT_FOLDER, \"tuned_confusion_matrix.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Sentiment analysis using VADER\n",
    "# -----------------------\n",
    "print(\"\\nRunning VADER sentiment analysis...\")\n",
    "try:\n",
    "    nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
    "except LookupError:\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ea89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sentiment scores\n",
    "sent_scores = df[\"Data\"].astype(str).apply(lambda t: sia.polarity_scores(t)[\"compound\"])\n",
    "def sentiment_label(c):\n",
    "    if c >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif c <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "df[\"sentiment_score\"] = sent_scores\n",
    "df[\"sentiment_label\"] = df[\"sentiment_score\"].apply(sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c9c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV with sentiment + predictions (merge predictions_df on text)\n",
    "# Attach baseline predictions where possible by index:\n",
    "# We already saved predictions_df for the test subset; let's save overall sentiment + labels for full data.\n",
    "df.to_csv(os.path.join(OUTPUT_FOLDER, \"blogs_with_sentiment.csv\"), index=False)\n",
    "print(\"Saved sentiment-annotated CSV to:\", os.path.join(OUTPUT_FOLDER, \"blogs_with_sentiment.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cb19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Summary JSON for assignment\n",
    "# -----------------------\n",
    "summary = {\n",
    "    \"n_documents\": int(len(df)),\n",
    "    \"n_classes\": int(len(le.classes_)),\n",
    "    \"classes\": list(le.classes_),\n",
    "    \"baseline_accuracy\": float(acc),\n",
    "    \"tuned_grid_best\": gs.best_params_,\n",
    "    \"tuned_cv_score\": float(gs.best_score_),\n",
    "    \"tuned_test_accuracy\": float(acc_gs)\n",
    "}\n",
    "with open(os.path.join(OUTPUT_FOLDER, \"nb_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f8d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAll done. Outputs saved to:\", OUTPUT_FOLDER)\n",
    "print(\"Key files:\")\n",
    "print(\" - baseline_classification_report.txt\")\n",
    "print(\" - baseline_confusion_matrix.csv/png\")\n",
    "print(\" - baseline_predictions.csv\")\n",
    "print(\" - nb_baseline.joblib\")\n",
    "print(\" - nb_tuned_pipeline.joblib (GridSearch best)\")\n",
    "print(\" - tuned_classification_report.txt\")\n",
    "print(\" - blogs_with_sentiment.csv\")\n",
    "print(\" - nb_summary.json\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
