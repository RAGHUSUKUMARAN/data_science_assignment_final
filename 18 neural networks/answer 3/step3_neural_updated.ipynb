{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe2323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3_neural.py\n",
    "\"\"\"\n",
    "ANN for Alphabets classification (baseline + random-search hyperparameter tuning + evaluation report)\n",
    "\n",
    "- Expects Alphabets_data.csv available at INPUT_PATH (change if needed)\n",
    "- Saves outputs (models, scaler/label encoder, metrics, plots, preds, tuning CSVs, evaluation report) to OUTPUT_FOLDER\n",
    "- Uses TensorFlow / Keras + scikit-learn\n",
    "\n",
    "Requirements:\n",
    "    pip install numpy pandas scikit-learn matplotlib tensorflow joblib\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb344df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26788a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad93deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Config - change if needed\n",
    "# -----------------------\n",
    "INPUT_PATH = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\18 neural networks\\Neural networks\\Alphabets_data.csv\"\n",
    "OUTPUT_FOLDER = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\18 neural networks\\Neural networks\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model training settings\n",
    "BASELINE_HIDDEN_UNITS = 128\n",
    "BASELINE_DROPOUT = 0.2\n",
    "BASELINE_LR = 1e-3\n",
    "BASELINE_BATCH = 64\n",
    "BASELINE_EPOCHS = 80\n",
    "BASELINE_ES_PATIENCE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e7d9a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Tuning settings\n",
    "RUN_TUNING = True         # Set False to skip random-search tuning\n",
    "TUNING_N_ITER = 20        # Increase to 50+ for a more thorough search\n",
    "TUNING_EPOCHS = 50\n",
    "TUNING_PATIENCE = 7\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714639a1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Utility functions\n",
    "# -----------------------\n",
    "def build_basic_ann(input_dim, num_classes, hidden_units=128, dropout_rate=0.2):\n",
    "    model = Sequential([\n",
    "        Dense(hidden_units, input_dim=input_dim, activation=\"relu\"),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4b173",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_model_with_hparams(input_dim, num_classes, num_layers, units, activation, dropout, learning_rate, optimizer_name):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units, activation=activation, input_dim=input_dim))\n",
    "    if dropout and dropout > 0.0:\n",
    "        model.add(Dropout(dropout))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        if dropout and dropout > 0.0:\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e624037",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_history(history, save_path):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history.get(\"loss\", []), label=\"train_loss\")\n",
    "    plt.plot(history.history.get(\"val_loss\", []), label=\"val_loss\")\n",
    "    plt.xlabel(\"epoch\"); plt.title(\"Loss\"); plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history.get(\"accuracy\", []), label=\"train_acc\")\n",
    "    plt.plot(history.history.get(\"val_accuracy\", []), label=\"val_acc\")\n",
    "    plt.xlabel(\"epoch\"); plt.title(\"Accuracy\"); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f91549",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_confusion_matrix(cm, labels, csv_path, png_path, title=\"Confusion matrix\"):\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm_df.to_csv(csv_path)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(png_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01ec99",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test_onehot, label_encoder, out_prefix):\n",
    "    \"\"\"\n",
    "    Evaluate a Keras model and write outputs:\n",
    "      - classification_report text\n",
    "      - confusion matrix CSV + PNG\n",
    "      - predictions CSV (true_label, pred_label, pred_confidence)\n",
    "    Returns a dict of aggregated metrics.\n",
    "    \"\"\"\n",
    "    # Predict\n",
    "    y_prob = model.predict(X_test)\n",
    "    y_pred_int = np.argmax(y_prob, axis=1)\n",
    "    y_true_int = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "    # Basic metrics\n",
    "    acc = accuracy_score(y_true_int, y_pred_int)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true_int, y_pred_int, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true_int, y_pred_int, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    # Per-class report\n",
    "    cls_report = classification_report(y_true_int, y_pred_int, target_names=label_encoder.classes_, digits=4)\n",
    "\n",
    "    # Save classification report\n",
    "    with open(f\"{out_prefix}_classification_report.txt\", \"w\") as f:\n",
    "        f.write(f\"Accuracy: {acc:.6f}\\n\\n\")\n",
    "        f.write(cls_report)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true_int, y_pred_int)\n",
    "    save_confusion_matrix(cm, label_encoder.classes_,\n",
    "                         f\"{out_prefix}_confusion_matrix.csv\",\n",
    "                         f\"{out_prefix}_confusion_matrix.png\",\n",
    "                         title=f\"{out_prefix} - Confusion matrix\")\n",
    "\n",
    "    # Save predictions\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"true_label\": label_encoder.inverse_transform(y_true_int),\n",
    "        \"pred_label\": label_encoder.inverse_transform(y_pred_int),\n",
    "        \"pred_confidence\": np.max(y_prob, axis=1)\n",
    "    })\n",
    "    pred_df.to_csv(f\"{out_prefix}_predictions.csv\", index=False)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision_macro\": float(precision_macro),\n",
    "        \"recall_macro\": float(recall_macro),\n",
    "        \"f1_macro\": float(f1_macro),\n",
    "        \"precision_weighted\": float(precision_weighted),\n",
    "        \"recall_weighted\": float(recall_weighted),\n",
    "        \"f1_weighted\": float(f1_weighted),\n",
    "        \"n_test_samples\": int(len(y_true_int))\n",
    "    }\n",
    "    # Save metrics JSON\n",
    "    with open(f\"{out_prefix}_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329a9de",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Main\n",
    "# -----------------------\n",
    "def main():\n",
    "    # reproducibility (best effort; TF nondeterminism may still occur)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset from:\", INPUT_PATH)\n",
    "    df = pd.read_csv(INPUT_PATH)\n",
    "    if \"letter\" not in df.columns:\n",
    "        raise ValueError(\"Target column 'letter' not found in the CSV.\")\n",
    "    print(f\"Loaded data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "    # Separate X, y\n",
    "    X = df.drop(columns=[\"letter\"])\n",
    "    y = df[\"letter\"]\n",
    "\n",
    "    # Encode target\n",
    "    le = LabelEncoder()\n",
    "    y_int = le.fit_transform(y)                 # integers 0..N-1\n",
    "    num_classes = len(le.classes_)\n",
    "    y_cat = tf.keras.utils.to_categorical(y_int, num_classes=num_classes)\n",
    "    print(f\"Detected {num_classes} classes: {list(le.classes_)}\")\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_cat, test_size=0.2, random_state=SEED, stratify=y_int\n",
    "    )\n",
    "    input_dim = X_train.shape[1]\n",
    "    print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "    # Save preprocessing objects\n",
    "    joblib.dump(scaler, os.path.join(OUTPUT_FOLDER, \"scaler.joblib\"))\n",
    "    joblib.dump(le, os.path.join(OUTPUT_FOLDER, \"label_encoder.joblib\"))\n",
    "\n",
    "    # -----------------------\n",
    "    # Baseline training\n",
    "    # -----------------------\n",
    "    print(\"\\n=== Baseline model training ===\")\n",
    "    baseline_model = build_basic_ann(input_dim, num_classes, BASELINE_HIDDEN_UNITS, BASELINE_DROPOUT)\n",
    "    baseline_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=BASELINE_LR),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    baseline_ckpt = os.path.join(OUTPUT_FOLDER, \"baseline_best_model.h5\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=BASELINE_ES_PATIENCE, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint(baseline_ckpt, monitor=\"val_loss\", save_best_only=True, verbose=1)\n",
    "    ]\n",
    "    history = baseline_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.15,\n",
    "        epochs=BASELINE_EPOCHS,\n",
    "        batch_size=BASELINE_BATCH,\n",
    "        callbacks=callbacks,\n",
    "        verbose=2\n",
    "    )\n",
    "    baseline_final_path = os.path.join(OUTPUT_FOLDER, \"baseline_final_model.h5\")\n",
    "    baseline_model.save(baseline_final_path)\n",
    "    print(f\"Baseline model saved: {baseline_final_path}\")\n",
    "\n",
    "    # Plot baseline history\n",
    "    plot_history(history, os.path.join(OUTPUT_FOLDER, \"baseline_training_history.png\"))\n",
    "\n",
    "    # Evaluate baseline on test set (and save artifacts)\n",
    "    baseline_prefix = os.path.join(OUTPUT_FOLDER, \"baseline\")\n",
    "    baseline_metrics = evaluate_model(baseline_model, X_test, y_test, le, baseline_prefix)\n",
    "    print(\"\\nBaseline metrics:\", baseline_metrics)\n",
    "\n",
    "    # -----------------------\n",
    "    # Hyperparameter tuning (Random Search)\n",
    "    # -----------------------\n",
    "    best_tuned_path = os.path.join(OUTPUT_FOLDER, \"best_tuned_model.h5\")\n",
    "    best_record = None\n",
    "\n",
    "    if RUN_TUNING:\n",
    "        print(\"\\n=== Starting Random Search hyperparameter tuning ===\")\n",
    "\n",
    "        # Tuning search space\n",
    "        search_space = {\n",
    "            \"num_layers\": [1, 2, 3],\n",
    "            \"units\": [32, 64, 128, 256],\n",
    "            \"activation\": [\"relu\", \"tanh\", \"elu\"],\n",
    "            \"dropout\": [0.0, 0.2, 0.4],\n",
    "            \"learning_rate\": [1e-4, 5e-4, 1e-3, 5e-3, 1e-2],\n",
    "            \"batch_size\": [32, 64, 128],\n",
    "            \"optimizer\": [\"adam\", \"sgd\"]\n",
    "        }\n",
    "\n",
    "        def sample_config(space):\n",
    "            return {k: random.choice(v) for k, v in space.items()}\n",
    "\n",
    "        best_test_acc = -1.0\n",
    "        results = []\n",
    "        tuning_results_csv = os.path.join(OUTPUT_FOLDER, \"hyperparam_tuning_results.csv\")\n",
    "\n",
    "        # For evaluation we need integer labels\n",
    "        y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "        for i in range(1, TUNING_N_ITER + 1):\n",
    "            cfg = sample_config(search_space)\n",
    "            print(f\"\\n--- Iter {i}/{TUNING_N_ITER} | cfg: {cfg} ---\")\n",
    "\n",
    "            model = build_model_with_hparams(\n",
    "                input_dim=input_dim,\n",
    "                num_classes=num_classes,\n",
    "                num_layers=cfg[\"num_layers\"],\n",
    "                units=cfg[\"units\"],\n",
    "                activation=cfg[\"activation\"],\n",
    "                dropout=cfg[\"dropout\"],\n",
    "                learning_rate=cfg[\"learning_rate\"],\n",
    "                optimizer_name=cfg[\"optimizer\"]\n",
    "            )\n",
    "\n",
    "            iter_ckpt = os.path.join(OUTPUT_FOLDER, f\"tmp_model_iter{i}.h5\")\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor=\"val_loss\", patience=TUNING_PATIENCE, restore_best_weights=True, verbose=0),\n",
    "                ModelCheckpoint(iter_ckpt, monitor=\"val_loss\", save_best_only=True, verbose=0)\n",
    "            ]\n",
    "\n",
    "            t0 = time.time()\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_split=0.15,\n",
    "                epochs=TUNING_EPOCHS,\n",
    "                batch_size=cfg[\"batch_size\"],\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            duration = time.time() - t0\n",
    "\n",
    "            # Evaluate on test set\n",
    "            y_pred_prob = model.predict(X_test)\n",
    "            y_pred_int = np.argmax(y_pred_prob, axis=1)\n",
    "            test_acc = accuracy_score(y_test_labels, y_pred_int)\n",
    "\n",
    "            val_acc = max(history.history.get(\"val_accuracy\", [0]))\n",
    "            val_loss = min(history.history.get(\"val_loss\", [float(\"inf\")]))\n",
    "\n",
    "            record = {\n",
    "                \"iter\": i,\n",
    "                \"num_layers\": cfg[\"num_layers\"],\n",
    "                \"units\": cfg[\"units\"],\n",
    "                \"activation\": cfg[\"activation\"],\n",
    "                \"dropout\": cfg[\"dropout\"],\n",
    "                \"learning_rate\": cfg[\"learning_rate\"],\n",
    "                \"optimizer\": cfg[\"optimizer\"],\n",
    "                \"batch_size\": cfg[\"batch_size\"],\n",
    "                \"val_loss\": float(val_loss),\n",
    "                \"val_acc\": float(val_acc),\n",
    "                \"test_acc\": float(test_acc),\n",
    "                \"train_epochs_ran\": len(history.history.get(\"loss\", [])),\n",
    "                \"duration_sec\": float(duration)\n",
    "            }\n",
    "            results.append(record)\n",
    "\n",
    "            # Save per-iteration history\n",
    "            hist_path = os.path.join(OUTPUT_FOLDER, f\"history_iter{i}.json\")\n",
    "            with open(hist_path, \"w\") as hf:\n",
    "                json.dump(history.history, hf)\n",
    "\n",
    "            print(f\"Iter {i} done | val_acc={val_acc:.4f} | test_acc={test_acc:.4f} | epochs={record['train_epochs_ran']} | {duration:.1f}s\")\n",
    "\n",
    "            # Save best model (by test acc)\n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "                best_record = record\n",
    "                model.save(best_tuned_path)\n",
    "                plot_history(history, os.path.join(OUTPUT_FOLDER, f\"best_history_iter{i}.png\"))\n",
    "                print(f\"New best model saved to {best_tuned_path} (test_acc={test_acc:.4f})\")\n",
    "\n",
    "        # Save tuning summary CSV\n",
    "        df_results = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False).reset_index(drop=True)\n",
    "        df_results.to_csv(tuning_results_csv, index=False)\n",
    "        print(\"\\n=== Random search complete ===\")\n",
    "        print(\"Best record (by test_acc):\")\n",
    "        print(best_record)\n",
    "        print(\"All tuning results saved to:\", tuning_results_csv)\n",
    "\n",
    "    else:\n",
    "        print(\"\\nRUN_TUNING is False: skipping hyperparameter search.\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Evaluation: compare baseline vs tuned (if available)\n",
    "    # -----------------------\n",
    "    evaluation_report_path = os.path.join(OUTPUT_FOLDER, \"evaluation_report.txt\")\n",
    "    summary_csv_path = os.path.join(OUTPUT_FOLDER, \"baseline_vs_tuned_summary.csv\")\n",
    "    summary_json_path = os.path.join(OUTPUT_FOLDER, \"baseline_vs_tuned_summary.json\")\n",
    "\n",
    "    report_lines = []\n",
    "    report_lines.append(\"Evaluation Report\\n\")\n",
    "    report_lines.append(f\"Dataset: {os.path.basename(INPUT_PATH)}\")\n",
    "    report_lines.append(f\"Output folder: {OUTPUT_FOLDER}\\n\")\n",
    "    report_lines.append(\"=== Baseline model metrics ===\")\n",
    "    for k, v in baseline_metrics.items():\n",
    "        report_lines.append(f\"{k}: {v}\")\n",
    "    report_lines.append(\"\")\n",
    "\n",
    "    tuned_metrics = None\n",
    "    if RUN_TUNING and best_record is not None and os.path.exists(best_tuned_path):\n",
    "        # load best tuned model and evaluate\n",
    "        print(\"\\nLoading best tuned model for evaluation:\", best_tuned_path)\n",
    "        tuned_model = load_model(best_tuned_path)\n",
    "        tuned_prefix = os.path.join(OUTPUT_FOLDER, \"tuned_best\")\n",
    "        tuned_metrics = evaluate_model(tuned_model, X_test, y_test, le, tuned_prefix)\n",
    "        report_lines.append(\"=== Tuned model metrics (best) ===\")\n",
    "        for k, v in tuned_metrics.items():\n",
    "            report_lines.append(f\"{k}: {v}\")\n",
    "        report_lines.append(\"\")\n",
    "        report_lines.append(\"Best hyperparameter record (from tuning):\")\n",
    "        report_lines.append(json.dumps(best_record, indent=2))\n",
    "        report_lines.append(\"\")\n",
    "    else:\n",
    "        report_lines.append(\"No tuned model available (tuning skipped or no best model saved).\")\n",
    "        report_lines.append(\"\")\n",
    "\n",
    "    # Compare baseline vs tuned (if tuned exists)\n",
    "    report_lines.append(\"=== Comparison summary ===\")\n",
    "    comparison_rows = []\n",
    "    headers = [\"metric\", \"baseline\", \"tuned\", \"delta (tuned - baseline)\"]\n",
    "    metrics_to_compare = [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\"]\n",
    "\n",
    "    for metric_name in metrics_to_compare:\n",
    "        base_val = baseline_metrics.get(metric_name, None)\n",
    "        tuned_val = tuned_metrics.get(metric_name, None) if tuned_metrics else None\n",
    "        delta = (tuned_val - base_val) if (base_val is not None and tuned_val is not None) else None\n",
    "        comparison_rows.append({\n",
    "            \"metric\": metric_name,\n",
    "            \"baseline\": base_val,\n",
    "            \"tuned\": tuned_val,\n",
    "            \"delta\": delta\n",
    "        })\n",
    "        line = f\"{metric_name}: baseline={base_val} | tuned={tuned_val} | delta={delta}\"\n",
    "        report_lines.append(line)\n",
    "\n",
    "    # Short discussion (automated template + pointers)\n",
    "    report_lines.append(\"\\nDiscussion / Observations:\")\n",
    "    if tuned_metrics:\n",
    "        acc_delta = tuned_metrics[\"accuracy\"] - baseline_metrics[\"accuracy\"]\n",
    "        report_lines.append(f\"- Overall test accuracy changed by {acc_delta:.6f} (tuned - baseline).\")\n",
    "        report_lines.append(\"- If tuned model shows improvement, likely causes include better learning rate / depth / regularization choices from random search.\")\n",
    "        report_lines.append(\"- If tuned model did not improve, possible reasons:\")\n",
    "        report_lines.append(\"  * search space did not cover the region with better hyperparameters\")\n",
    "        report_lines.append(\"  * insufficient search budget (increase TUNING_N_ITER)\")\n",
    "        report_lines.append(\"  * model architecture capacity / dataset size mismatch\")\n",
    "        report_lines.append(\"- Recommendations:\")\n",
    "        report_lines.append(\"  * run a focused local grid around the best lr/units found\")\n",
    "        report_lines.append(\"  * try Keras Tuner (Hyperband or Bayesian) for smarter sampling\")\n",
    "        report_lines.append(\"  * consider data augmentation, feature engineering, or deeper architectures if underfitting\")\n",
    "    else:\n",
    "        report_lines.append(\"- No tuned model to compare; baseline metrics reported above.\")\n",
    "        report_lines.append(\"- To get a tuned model: set RUN_TUNING = True and increase TUNING_N_ITER.\")\n",
    "\n",
    "    # Save evaluation report\n",
    "    with open(evaluation_report_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(report_lines))\n",
    "\n",
    "    # Save CSV and JSON comparison\n",
    "    df_comp = pd.DataFrame(comparison_rows)\n",
    "    df_comp.to_csv(summary_csv_path, index=False)\n",
    "    with open(summary_json_path, \"w\") as f:\n",
    "        json.dump({\"baseline_metrics\": baseline_metrics, \"tuned_metrics\": tuned_metrics, \"comparison\": comparison_rows, \"best_record\": best_record}, f, indent=2)\n",
    "\n",
    "    print(\"\\nEvaluation complete.\")\n",
    "    print(\"Saved evaluation report to:\", evaluation_report_path)\n",
    "    print(\"Saved comparison CSV to:\", summary_csv_path)\n",
    "    print(\"Saved summary JSON to:\", summary_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8feff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
