{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7afd08c1",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Conclusion & Recommendations\n",
    "\n",
    "From the evaluation metrics (R² and RMSE), the performance summary is as follows:\n",
    "\n",
    "| Model | R² (↑ better) | RMSE (↓ better) | Remarks |\n",
    "|:------|:--------------:|:---------------:|:--------|\n",
    "| **Linear Regression** | ~0.852 | ~1403.6 | Simple and interpretable baseline model |\n",
    "| **Polynomial Regression (degree=2)** | ~0.910 | ~1095.7 | Best predictive performance, but higher model complexity |\n",
    "| **Ridge Regression** | ~0.852 | ~1403.7 | Similar to Linear; helps with multicollinearity |\n",
    "| **Lasso Regression** | ~0.852 | ~1403.6 | Similar to Ridge; adds feature selection (sparse coefficients) |\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Polynomial Regression** achieved the **highest R² (≈0.91)** and the **lowest RMSE (≈1095)**, indicating it captures non-linear relationships in the data effectively.  \n",
    "- **Linear Regression**, while simpler, still performs decently and is easier to explain — suitable if interpretability is prioritized.  \n",
    "- **Ridge and Lasso** provided minimal performance gains, implying the dataset doesn’t suffer heavily from overfitting or multicollinearity.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Recommendation\n",
    "\n",
    "- For **predictive accuracy**, **Polynomial Regression** is the most suitable model.  \n",
    "- For **interpretability and production simplicity**, **Linear Regression** remains a solid choice.  \n",
    "- Ridge and Lasso are beneficial for regularization but not strictly necessary here.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "> The price of used Toyota cars is most strongly influenced by **Age, Weight, KM, and Horsepower**.  \n",
    "> Polynomial relationships improve prediction accuracy, suggesting that depreciation and wear effects are **non-linear** in nature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698aa1c5",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Conclusion & Next Steps\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "This project focused on predicting **used Toyota Corolla car prices** using multiple regression models.  \n",
    "After cleaning, encoding, and scaling the dataset, we evaluated several algorithms:\n",
    "\n",
    "| Model | R² (↑ better) | RMSE (↓ better) | Remarks |\n",
    "|:------|:--------------:|:---------------:|:--------|\n",
    "| **Linear Regression** | ~0.852 | ~1403.6 | Simple, transparent, and interpretable baseline |\n",
    "| **Polynomial Regression (degree=2)** | ~0.910 | ~1095.7 | Best overall performance, captures non-linearity effectively |\n",
    "| **Ridge Regression** | ~0.852 | ~1403.7 | Stabilizes coefficients, minimal improvement |\n",
    "| **Lasso Regression** | ~0.852 | ~1403.6 | Adds feature selection; performs similarly to Ridge |\n",
    "\n",
    "**Key Influencing Factors:**  \n",
    "- **Age_08_04 (Car Age)** — strongest negative impact on price (older cars depreciate).  \n",
    "- **Weight** — positively correlated with price (heavier, often higher-end models).  \n",
    "- **KM** — higher mileage reduces price significantly.  \n",
    "- **HP** — higher horsepower slightly increases resale value.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps / Future Improvements\n",
    "\n",
    "To further refine the model and make it deployment-ready:\n",
    "\n",
    "1. **Hyperparameter Tuning:**  \n",
    "   Use `GridSearchCV` or `RandomizedSearchCV` to optimize Ridge/Lasso α values, polynomial degree, and cross-validation folds.\n",
    "\n",
    "2. **Cross-Validation:**  \n",
    "   Implement `KFold` or `cross_val_score` to ensure model generalization and prevent overfitting.\n",
    "\n",
    "3. **Pipeline Creation:**  \n",
    "   Build a complete `sklearn.pipeline` integrating preprocessing, scaling, and model training for reproducibility.\n",
    "\n",
    "4. **Feature Engineering:**  \n",
    "   - Introduce interaction terms (e.g., `Age × KM`) for nuanced relationships.  \n",
    "   - Test log-transforms for skewed variables like `Price` or `KM`.\n",
    "\n",
    "5. **Model Export:**  \n",
    "   Save the trained model using `joblib` or `pickle`:\n",
    "   ```python\n",
    "   import joblib\n",
    "   joblib.dump(best_model, 'toyota_price_model.pkl')\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### Final Note\n",
    "\n",
    "The final model demonstrates that **car depreciation is not purely linear**, and incorporating non-linear terms greatly improves accuracy.  \n",
    "Further validation and tuning can make this model production-ready for **used car price prediction systems**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
