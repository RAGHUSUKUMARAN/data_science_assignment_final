{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "full_outlier_pipeline.py\n",
    "\n",
    "Usage:\n",
    "    python full_outlier_pipeline.py\n",
    "\n",
    "What it does:\n",
    "- Looks for adult_engineered.csv in your folder. If missing:\n",
    "    - tries to load adult_encoded.csv and add missing engineered columns\n",
    "    - if that is missing, tries to load the raw adult_with_headers.csv and runs basic encoding+engineering\n",
    "- Performs IsolationForest outlier detection on selected numeric columns\n",
    "- Saves adult_engineered.csv and adult_no_outliers.csv to the folder:\n",
    "    D:\\DATA SCIENCE\\ASSIGNMENTS\\12 EDA2\\EDA2\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e7563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG: change if your folder is different ===\n",
    "BASE_DIR = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\12 EDA2\\EDA2\"\n",
    "ENGINEERED_PATH = os.path.join(BASE_DIR, \"adult_engineered.csv\")\n",
    "ENCODED_PATH = os.path.join(BASE_DIR, \"adult_encoded.csv\")\n",
    "RAW_PATH     = os.path.join(BASE_DIR, \"adult_with_headers.csv\")\n",
    "OUTLIER_PATH = os.path.join(BASE_DIR, \"adult_no_outliers.csv\")\n",
    "# =================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b113b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_csv_safe(path):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Loading: {path}\")\n",
    "        return pd.read_csv(path)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c417140",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def basic_encoding_from_raw(df):\n",
    "    \"\"\"\n",
    "    Minimal encoding of raw Adult dataset:\n",
    "    - Replace '?' -> NaN, fill with 'Unknown' for categoricals\n",
    "    - One-hot encode sex and race (as per instructions)\n",
    "    - Label-encode other categorical cols with >5 categories\n",
    "    - Map target income to 0/1\n",
    "    Returns encoded dataframe\n",
    "    \"\"\"\n",
    "    print(\"Performing basic encoding from raw dataset...\")\n",
    "    df = df.copy()\n",
    "    # normalize whitespace and replace '?' with NaN\n",
    "    df = df.replace(' ?', np.nan)\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    # fill missing categorical with 'Unknown'\n",
    "    cat_cols = ['workclass','education','marital_status','occupation','relationship','race','sex','native_country']\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna('Unknown')\n",
    "    # One-hot encode sex and race\n",
    "    for c in ['sex','race']:\n",
    "        if c in df.columns:\n",
    "            # keep all columns (do not drop first); easier to track\n",
    "            df = pd.get_dummies(df, columns=[c], prefix=c)\n",
    "    # Label encode other categorical features (except ones just one-hot encoded and target)\n",
    "    label_cols = [c for c in cat_cols if c in df.columns and not c.startswith('sex') and not c.startswith('race')]\n",
    "    # Remove 'sex' and 'race' if they are now one-hot columns names\n",
    "    label_cols = [c for c in label_cols if c not in ['sex','race']]\n",
    "    le = LabelEncoder()\n",
    "    for c in label_cols:\n",
    "        # convert to str to ensure consistent encoding\n",
    "        df[c] = le.fit_transform(df[c].astype(str))\n",
    "    # target encode\n",
    "    if 'income' in df.columns:\n",
    "        df['income'] = df['income'].map({'<=50K': 0, '<=50K.': 0, '>50K': 1, '>50K.': 1})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845276e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ensure_engineered(df):\n",
    "    \"\"\"\n",
    "    Ensure the engineered features exist:\n",
    "    - age_group (Young, Middle-aged, Senior)\n",
    "    - work_hours_cat (Part-time, Full-time, Over-time, Heavy Workload)\n",
    "    - capital_gain_log, capital_loss_log\n",
    "    If these cols exist already, function will not overwrite them.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Age group\n",
    "    if 'age_group' not in df.columns:\n",
    "        print(\"Creating 'age_group'...\")\n",
    "        df['age_group'] = pd.cut(df['age'], bins=[0,30,50,200], labels=['Young','Middle-aged','Senior'])\n",
    "    else:\n",
    "        print(\"'age_group' exists; skipping creation.\")\n",
    "    # Work hours category\n",
    "    if 'work_hours_cat' not in df.columns:\n",
    "        print(\"Creating 'work_hours_cat'...\")\n",
    "        df['work_hours_cat'] = pd.cut(df['hours_per_week'], bins=[0,30,40,60,200], labels=['Part-time','Full-time','Over-time','Heavy Workload'])\n",
    "    else:\n",
    "        print(\"'work_hours_cat' exists; skipping creation.\")\n",
    "    # Log transforms for capital gain/loss\n",
    "    if 'capital_gain_log' not in df.columns:\n",
    "        if 'capital_gain' in df.columns:\n",
    "            print(\"Creating 'capital_gain_log'...\")\n",
    "            df['capital_gain_log'] = np.log1p(df['capital_gain'].fillna(0).astype(float))\n",
    "        else:\n",
    "            print(\"Warning: 'capital_gain' not found; skipping capital_gain_log.\")\n",
    "    else:\n",
    "        print(\"'capital_gain_log' exists; skipping creation.\")\n",
    "    if 'capital_loss_log' not in df.columns:\n",
    "        if 'capital_loss' in df.columns:\n",
    "            print(\"Creating 'capital_loss_log'...\")\n",
    "            df['capital_loss_log'] = np.log1p(df['capital_loss'].fillna(0).astype(float))\n",
    "        else:\n",
    "            print(\"Warning: 'capital_loss' not found; skipping capital_loss_log.\")\n",
    "    else:\n",
    "        print(\"'capital_loss_log' exists; skipping creation.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f7be6e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1) Try engineered first\n",
    "    df = load_csv_safe(ENGINEERED_PATH)\n",
    "    if df is not None:\n",
    "        print(\"Found engineered dataset. Verifying engineered columns...\")\n",
    "        df = ensure_engineered(df)\n",
    "    else:\n",
    "        # 2) Try encoded\n",
    "        df = load_csv_safe(ENCODED_PATH)\n",
    "        if df is not None:\n",
    "            print(\"Found encoded dataset. Adding engineered columns if missing...\")\n",
    "            df = ensure_engineered(df)\n",
    "        else:\n",
    "            # 3) Try raw\n",
    "            df = load_csv_safe(RAW_PATH)\n",
    "            if df is not None:\n",
    "                print(\"Found raw dataset. Running basic encoding + feature engineering...\")\n",
    "                df = basic_encoding_from_raw(df)\n",
    "                df = ensure_engineered(df)\n",
    "            else:\n",
    "                print(\"ERROR: None of the expected files were found in the folder:\")\n",
    "                print(f\"  - {ENGINEERED_PATH}\")\n",
    "                print(f\"  - {ENCODED_PATH}\")\n",
    "                print(f\"  - {RAW_PATH}\")\n",
    "                print(\"Place one of these files in the folder and re-run.\")\n",
    "                sys.exit(1)\n",
    "\n",
    "    # Save engineered dataset (overwrite or create)\n",
    "    try:\n",
    "        df.to_csv(ENGINEERED_PATH, index=False)\n",
    "        print(f\"Saved engineered dataset to: {ENGINEERED_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to save engineered CSV:\", e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --------------- Isolation Forest Outlier Detection ---------------\n",
    "    # Prepare numeric cols for outlier detection; if log columns are present prefer them\n",
    "    numeric_candidates = []\n",
    "    # prefer log versions if available\n",
    "    for c in ['age','fnlwgt','education_num','capital_gain_log','capital_loss_log','hours_per_week']:\n",
    "        if c in df.columns:\n",
    "            numeric_candidates.append(c)\n",
    "    if not numeric_candidates:\n",
    "        print(\"No numeric columns found for outlier detection. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    print(\"Numeric columns used for outlier detection:\", numeric_candidates)\n",
    "\n",
    "    # Drop rows with NaN in numeric candidates (IsolationForest cannot handle NaN)\n",
    "    df_num = df[numeric_candidates].copy()\n",
    "    nan_count = df_num.isnull().any(axis=1).sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Found {nan_count} rows with NaN in numeric columns; dropping those rows for outlier detection.\")\n",
    "        valid_idx = ~df_num.isnull().any(axis=1)\n",
    "        df_for_iso = df.loc[valid_idx, :].copy()\n",
    "    else:\n",
    "        df_for_iso = df.copy()\n",
    "\n",
    "    X = df_for_iso[numeric_candidates].astype(float).values\n",
    "\n",
    "    # Configure Isolation Forest\n",
    "    contamination = 0.02  # you can adjust this; 0.02 means ~2% anomalies\n",
    "    iso = IsolationForest(contamination=contamination, random_state=42)\n",
    "    print(\"Fitting IsolationForest...\")\n",
    "    iso.fit(X)\n",
    "    preds = iso.predict(X)  # -1 for outlier, 1 for inlier\n",
    "\n",
    "    # Keep only inliers\n",
    "    inlier_mask = (preds == 1)\n",
    "    df_inliers = df_for_iso.loc[inlier_mask, :].copy()\n",
    "\n",
    "    # If we dropped rows earlier due to NaN, combine with rows that were excluded (we choose to exclude them from final dataset too)\n",
    "    final_df = df_inliers.copy()\n",
    "\n",
    "    # Save outlier-removed dataset\n",
    "    try:\n",
    "        final_df.to_csv(OUTLIER_PATH, index=False)\n",
    "        print(f\"Saved outlier-removed dataset to: {OUTLIER_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to save outlier-removed CSV:\", e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"Original rows:\", len(df), \"→ After removing outliers:\", len(final_df))\n",
    "    print(\"Done ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
