{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "feature_correlations.py\n",
    "Full pipeline to investigate feature correlations (robust loader + debug prints).\n",
    "Drop this file into your project and run with your venv python.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa681afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency, pointbiserialr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70805d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- User config ----------\n",
    "CSV_PATH = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\17 SVM\\SVM\\mushroom.csv\"\n",
    "TARGET_COL = None\n",
    "OUTPUT_DIR = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\17 SVM\\SVM\\correlation_outputs\"\n",
    "DROP_THRESHOLD_NUNIQUE = 1\n",
    "FILLNA_STRATEGY = \"mode\"\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cdf613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick sanity checks\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"CSV_PATH (raw):\", CSV_PATH)\n",
    "print(\"CSV_PATH (absolute):\", os.path.abspath(CSV_PATH))\n",
    "print(\"CSV_PATH exists?\", os.path.exists(CSV_PATH))\n",
    "print(\"Readable by current user?\", os.access(os.path.abspath(CSV_PATH), os.R_OK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176da800",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31b6d3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------- Robust load ----------\n",
    "def try_read_csv(path):\n",
    "    \"\"\"Try several common encodings/separators and return (df, used_params) or raise.\"\"\"\n",
    "    attempts = [\n",
    "        {\"sep\": \",\", \"encoding\": \"utf-8\"},\n",
    "        {\"sep\": \",\", \"encoding\": \"latin1\"},\n",
    "        {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
    "        {\"sep\": \"\\t\", \"encoding\": \"utf-8\"},\n",
    "    ]\n",
    "    last_exc = None\n",
    "    for params in attempts:\n",
    "        try:\n",
    "            df = pd.read_csv(path, **params)\n",
    "            return df, params\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "    # final fallback: let pandas infer with engine python (slower but forgiving)\n",
    "    try:\n",
    "        df = pd.read_csv(path, engine=\"python\")\n",
    "        return df, {\"engine\": \"python\"}\n",
    "    except Exception as e:\n",
    "        raise last_exc or e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a686b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If user provided a DataFrame in the environment (rare here), use that\n",
    "df = globals().get(\"mushroom_df\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a90bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is None:\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        sys.exit(f\"File not found: {os.path.abspath(CSV_PATH)}\\nCheck path, spelling, and that the drive is accessible.\")\n",
    "    try:\n",
    "        df, used = try_read_csv(CSV_PATH)\n",
    "        print(\"Loaded CSV successfully with params:\", used)\n",
    "    except PermissionError as pe:\n",
    "        sys.exit(f\"Permission error reading file: {pe}\\nCheck file permissions.\")\n",
    "    except Exception as e:\n",
    "        # show full info to help debugging\n",
    "        import traceback\n",
    "        tb = traceback.format_exc()\n",
    "        sys.exit(f\"Failed to read CSV. Last exception:\\n{e}\\n\\nTraceback:\\n{tb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15579748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic confirmation\n",
    "print(\"Dataframe shape:\", getattr(df, \"shape\", None))\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head().to_string(index=False))\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Basic cleaning ----------\n",
    "try:\n",
    "    nunique = df.nunique(dropna=True)\n",
    "    const_cols = list(nunique[nunique <= DROP_THRESHOLD_NUNIQUE].index)\n",
    "    if const_cols:\n",
    "        print(f\"Dropping constant / low-variance columns: {const_cols}\")\n",
    "        df = df.drop(columns=const_cols)\n",
    "except Exception as e:\n",
    "    print(\"Warning during dropping constant columns:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NAs simply (user can adjust)\n",
    "if FILLNA_STRATEGY == \"mode\":\n",
    "    for c in df.columns:\n",
    "        if df[c].isna().any():\n",
    "            try:\n",
    "                df[c].fillna(df[c].mode().iloc[0], inplace=True)\n",
    "            except Exception:\n",
    "                df[c].fillna(method=\"ffill\", inplace=True)\n",
    "elif FILLNA_STRATEGY == \"median\":\n",
    "    for c in df.select_dtypes(include=[np.number]).columns:\n",
    "        if df[c].isna().any():\n",
    "            df[c].fillna(df[c].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bbc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic dtype coercion for mostly-numeric object columns\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == \"object\":\n",
    "        coerced = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        if coerced.notna().sum() / len(coerced) > 0.6:\n",
    "            df[col] = coerced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c48ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Split numeric & categorical ----------\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bcfc14",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"\\nNumeric columns ({len(num_cols)}): {num_cols}\")\n",
    "print(f\"Categorical columns ({len(cat_cols)}): {cat_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619935a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------- Helpers ----------\n",
    "def savefig_and_show(fig, fname):\n",
    "    path = os.path.join(OUTPUT_DIR, fname)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=150)\n",
    "    print(f\"Saved figure to {path}\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01b1ca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def cramers_v(series_x, series_y):\n",
    "    confusion = pd.crosstab(series_x, series_y)\n",
    "    if confusion.size == 0:\n",
    "        return np.nan\n",
    "    chi2, p, dof, expected = chi2_contingency(confusion)\n",
    "    n = confusion.sum().sum()\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion.shape\n",
    "    phi2corr = max(0, phi2 - ((k - 1)*(r - 1)) / (n - 1))\n",
    "    rcorr = r - ((r - 1)**2) / (n - 1)\n",
    "    kcorr = k - ((k - 1)**2) / (n - 1)\n",
    "    denom = min(kcorr - 1, rcorr - 1)\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return np.sqrt(phi2corr / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cee7be",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def correlation_ratio(categories, measurements):\n",
    "    categories = pd.Series(categories)\n",
    "    measurements = pd.Series(measurements)\n",
    "    mask = categories.notna() & measurements.notna()\n",
    "    categories = categories[mask]\n",
    "    measurements = measurements[mask]\n",
    "    if len(measurements) == 0:\n",
    "        return np.nan\n",
    "    cat_groups = measurements.groupby(categories)\n",
    "    mean_total = measurements.mean()\n",
    "    ss_between = sum([(grp.size * (grp.mean() - mean_total)**2) for _, grp in cat_groups])\n",
    "    ss_total = ((measurements - mean_total)**2).sum()\n",
    "    if ss_total == 0:\n",
    "        return 0.0\n",
    "    return np.sqrt(ss_between / ss_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Numeric correlations ----------\n",
    "if num_cols:\n",
    "    pearson = df[num_cols].corr(method=\"pearson\")\n",
    "    spearman = df[num_cols].corr(method=\"spearman\")\n",
    "    pearson.to_csv(os.path.join(OUTPUT_DIR, \"pearson_correlation_matrix.csv\"))\n",
    "    spearman.to_csv(os.path.join(OUTPUT_DIR, \"spearman_correlation_matrix.csv\"))\n",
    "    print(\"Saved numeric correlation matrices (pearson, spearman).\")\n",
    "    fig, ax = plt.subplots(figsize=(max(6, len(num_cols)*0.5), max(4, len(num_cols)*0.5)))\n",
    "    sns.heatmap(pearson, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=False,\n",
    "                cbar_kws={'shrink': .6}, linewidths=.5)\n",
    "    ax.set_title(\"Pearson Correlation (Numeric features)\")\n",
    "    savefig_and_show(fig, \"pearson_heatmap.png\")\n",
    "    pairs = []\n",
    "    for a, b in combinations(num_cols, 2):\n",
    "        pairs.append((a, b, pearson.loc[a, b]))\n",
    "    top_abs = sorted(pairs, key=lambda x: -abs(x[2]))[:20]\n",
    "    top_df = pd.DataFrame(top_abs, columns=[\"feature_a\", \"feature_b\", \"pearson_corr\"])\n",
    "    top_df.to_csv(os.path.join(OUTPUT_DIR, \"top_numeric_pairs_by_abs_pearson.csv\"), index=False)\n",
    "    print(\"Saved top numeric correlated pairs.\")\n",
    "else:\n",
    "    print(\"No numeric columns found; skipping numeric correlation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Categorical vs Categorical (Cramér's V) ----------\n",
    "if len(cat_cols) >= 2:\n",
    "    cramers_matrix = pd.DataFrame(index=cat_cols, columns=cat_cols, dtype=float)\n",
    "    for a, b in combinations(cat_cols, 2):\n",
    "        v = cramers_v(df[a], df[b])\n",
    "        cramers_matrix.loc[a, b] = v\n",
    "        cramers_matrix.loc[b, a] = v\n",
    "    np.fill_diagonal(cramers_matrix.values, 1.0)\n",
    "    cramers_matrix = cramers_matrix.fillna(0.0).astype(float)\n",
    "    cramers_matrix.to_csv(os.path.join(OUTPUT_DIR, \"cramers_v_matrix.csv\"))\n",
    "    print(\"Saved Cramér's V matrix for categorical features.\")\n",
    "    fig, ax = plt.subplots(figsize=(max(6, len(cat_cols)*0.35), max(6, len(cat_cols)*0.35)))\n",
    "    sns.heatmap(cramers_matrix, annot=True, fmt=\".2f\", cmap=\"vlag\", linewidths=.3)\n",
    "\n",
    "    ax.set_title(\"Cramér's V (Categorical vs Categorical)\")\n",
    "    savefig_and_show(fig, \"cramers_v_heatmap.png\")\n",
    "    cat_pairs = []\n",
    "    for a, b in combinations(cat_cols, 2):\n",
    "        cat_pairs.append((a, b, cramers_matrix.loc[a, b]))\n",
    "    top_cat = sorted(cat_pairs, key=lambda x: -x[2])[:30]\n",
    "    pd.DataFrame(top_cat, columns=[\"cat_a\", \"cat_b\", \"cramers_v\"]).to_csv(\n",
    "        os.path.join(OUTPUT_DIR, \"top_categorical_pairs_by_cramers.csv\"), index=False)\n",
    "    print(\"Saved top categorical pairs by Cramér's V.\")\n",
    "else:\n",
    "    print(\"Not enough categorical columns for Cramér's V (need >=2).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Categorical -> Numeric (Correlation ratio) ----------\n",
    "if cat_cols and num_cols:\n",
    "    eta_matrix = pd.DataFrame(index=cat_cols, columns=num_cols, dtype=float)\n",
    "    for c in cat_cols:\n",
    "        for n in num_cols:\n",
    "            eta_matrix.loc[c, n] = correlation_ratio(df[c], df[n])\n",
    "    eta_matrix = eta_matrix.fillna(0.0).astype(float)\n",
    "    eta_matrix.to_csv(os.path.join(OUTPUT_DIR, \"eta_correlation_ratio_matrix.csv\"))\n",
    "    print(\"Saved correlation ratio (eta) matrix for categorical->numeric.\")\n",
    "    fig, ax = plt.subplots(figsize=(max(6, len(num_cols)*0.5), max(4, len(cat_cols)*0.25)))\n",
    "    sns.heatmap(eta_matrix, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.3)\n",
    "    ax.set_title(\"Correlation Ratio (categorical -> numeric) η\")\n",
    "    savefig_and_show(fig, \"eta_heatmap.png\")\n",
    "    top_eta_rows = []\n",
    "    for c in cat_cols:\n",
    "        row = eta_matrix.loc[c].sort_values(ascending=False)[:10]\n",
    "        for n, val in row.items():\n",
    "            top_eta_rows.append((c, n, val))\n",
    "    pd.DataFrame(top_eta_rows, columns=[\"categorical\", \"numeric\", \"eta\"]).to_csv(\n",
    "        os.path.join(OUTPUT_DIR, \"top_categorical_to_numeric_eta.csv\"), index=False)\n",
    "    print(\"Saved categorical -> numeric top explanations (eta).\")\n",
    "else:\n",
    "    print(\"Skipping categorical->numeric eta matrix (need both categorical and numeric columns).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbb3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Extra: numeric vs binary categorical using point-biserial (if present) ----------\n",
    "binary_cat = [c for c in cat_cols if df[c].nunique() == 2]\n",
    "if binary_cat and num_cols:\n",
    "    pb_list = []\n",
    "    for c in binary_cat:\n",
    "        values = pd.Categorical(df[c]).codes\n",
    "        for n in num_cols:\n",
    "            try:\n",
    "                r, p = pointbiserialr(values, df[n])\n",
    "                pb_list.append((c, n, r, p))\n",
    "            except Exception:\n",
    "                pb_list.append((c, n, np.nan, np.nan))\n",
    "    pb_df = pd.DataFrame(pb_list, columns=[\"binary_cat\", \"numeric\", \"pointbiserial_r\", \"p_value\"])\n",
    "    pb_df.to_csv(os.path.join(OUTPUT_DIR, \"pointbiserial_binary_cat_numeric.csv\"), index=False)\n",
    "    print(\"Saved point-biserial correlations for binary categorical features.\")\n",
    "else:\n",
    "    print(\"No binary categorical columns or no numeric columns found; skipping point-biserial step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Summary output: top correlations consolidated ----------\n",
    "summary_rows = []\n",
    "if num_cols:\n",
    "    for _, r in top_df.iterrows():\n",
    "        summary_rows.append({\n",
    "            \"type\": \"numeric-numeric\",\n",
    "            \"a\": r['feature_a'],\n",
    "            \"b\": r['feature_b'],\n",
    "            \"score\": r['pearson_corr']\n",
    "        })\n",
    "if len(cat_cols) >= 2:\n",
    "    for row in top_cat:\n",
    "        summary_rows.append({\n",
    "            \"type\": \"cat-cat\",\n",
    "            \"a\": row[0],\n",
    "            \"b\": row[1],\n",
    "            \"score\": row[2]\n",
    "        })\n",
    "if cat_cols and num_cols:\n",
    "    for c, n, val in top_eta_rows:\n",
    "        summary_rows.append({\n",
    "            \"type\": \"cat->num\",\n",
    "            \"a\": c,\n",
    "            \"b\": n,\n",
    "            \"score\": val\n",
    "        })\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(by=\"score\", key=lambda col: col.abs(), ascending=False)\n",
    "summary_df.to_csv(os.path.join(OUTPUT_DIR, \"consolidated_top_correlations.csv\"), index=False)\n",
    "print(f\"Saved consolidated top correlations to {os.path.join(OUTPUT_DIR, 'consolidated_top_correlations.csv')}\")\n",
    "print(\"\\nDONE — All outputs are in the folder:\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
