{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_evaluation_heart.py\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PATHS ===\n",
    "BASE_PATH = r\"D:\\DATA SCIENCE\\ASSIGNMENTS\\13 decision tree\\Decision Tree\"\n",
    "PROCESSED_CSV = os.path.join(BASE_PATH, \"heart_processed.csv\")\n",
    "MODEL_PATH = os.path.join(BASE_PATH, \"decision_tree_best.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OUTPUT FILES ===\n",
    "EVAL_TXT = os.path.join(BASE_PATH, \"decision_tree_final_evaluation.txt\")\n",
    "CM_PNG = os.path.join(BASE_PATH, \"confusion_matrix_final.png\")\n",
    "ROC_PNG = os.path.join(BASE_PATH, \"roc_curve_final.png\")\n",
    "FI_PNG = os.path.join(BASE_PATH, \"feature_importances_final.png\")\n",
    "TREE_PNG = os.path.join(BASE_PATH, \"decision_tree_final.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ee421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD MODEL & DATA ===\n",
    "print(\"Loading model and data...\")\n",
    "df = pd.read_csv(PROCESSED_CSV)\n",
    "model = joblib.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2875eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PREPARE FEATURES ===\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adcc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MAKE PREDICTIONS ===\n",
    "y_pred = model.predict(X)\n",
    "y_proba = model.predict_proba(X)[:, 1] if hasattr(model, \"predict_proba\") else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1146982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === METRICS ===\n",
    "acc = accuracy_score(y, y_pred)\n",
    "prec = precision_score(y, y_pred, zero_division=0)\n",
    "rec = recall_score(y, y_pred, zero_division=0)\n",
    "f1 = f1_score(y, y_pred, zero_division=0)\n",
    "roc_auc = roc_auc_score(y, y_proba) if y_proba is not None else float(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAVE METRICS REPORT ===\n",
    "with open(EVAL_TXT, \"w\") as f:\n",
    "    f.write(\"=== Decision Tree Final Evaluation ===\\n\\n\")\n",
    "    f.write(f\"Accuracy: {acc:.4f}\\nPrecision: {prec:.4f}\\nRecall: {rec:.4f}\\nF1-score: {f1:.4f}\\nROC-AUC: {roc_auc:.4f}\\n\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(np.array2string(cm))\n",
    "    f.write(\"\\n\\nClassification Report:\\n\")\n",
    "    f.write(classification_report(y, y_pred, zero_division=0))\n",
    "print(\"Saved evaluation metrics to:\", EVAL_TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82226a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFUSION MATRIX PLOT ===\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix — Decision Tree\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(CM_PNG)\n",
    "plt.close()\n",
    "print(\"Saved confusion matrix plot:\", CM_PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e65412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ROC CURVE ===\n",
    "if y_proba is not None:\n",
    "    fpr, tpr, _ = roc_curve(y, y_proba)\n",
    "    roc_auc_val = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"ROC curve (area = {roc_auc_val:.3f})\")\n",
    "    plt.plot([0,1], [0,1], linestyle='--', lw=1, color='grey')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve — Decision Tree\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ROC_PNG)\n",
    "    plt.close()\n",
    "    print(\"Saved ROC curve:\", ROC_PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e894e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE IMPORTANCES ===\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=importances.head(15), y=importances.head(15).index)\n",
    "plt.title(\"Top 15 Feature Importances — Decision Tree\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FI_PNG)\n",
    "plt.close()\n",
    "print(\"Saved feature importance chart:\", FI_PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fffad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DECISION TREE STRUCTURE ===\n",
    "plt.figure(figsize=(22,12))\n",
    "plot_tree(model, feature_names=X.columns, class_names=[\"No Disease\", \"Disease\"],\n",
    "          filled=True, rounded=True, fontsize=8)\n",
    "plt.title(\"Decision Tree Structure — Final Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(TREE_PNG)\n",
    "plt.close()\n",
    "print(\"Saved decision tree visualization:\", TREE_PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SUMMARY IN CONSOLE ===\n",
    "print(\"\\n=== Model Performance Summary ===\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(\"\\nFeature Importances (Top 10):\")\n",
    "print(importances.head(10))\n",
    "print(\"\\nAll evaluation files are saved in:\", BASE_PATH)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
